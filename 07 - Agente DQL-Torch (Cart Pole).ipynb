{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cart Pole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Descripcion \n",
    "\n",
    "\n",
    "+ **Espacio de observacion (Box(4,))**\n",
    "\n",
    "+ Posicion del carro: [-2.4, 2.4]\n",
    "+ Velocidad del carro: [$-\\infty$, $\\infty$]\n",
    "+ Angulo del palo: [-41.8, 41.8]\n",
    "+ Velocidad del palo en la punta: [$-\\infty$, $\\infty$]\n",
    "\n",
    "\n",
    "+ **Espacio de accion (Discrete(2))**\n",
    "\n",
    "+ Izquierda: 0\n",
    "+ Derecha: 1\n",
    "\n",
    "\n",
    "El objetivo es mantener el palo vertical moviendo a izquierda y derecha el carro.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perceptron implementado en Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Perceptron(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, dim_entrada, dim_salida, hardware=torch.device('cpu')):\n",
    "        super(Perceptron, self).__init__()                               # herencia\n",
    "        self.hardware=hardware                                           # hardware usado por el perceptron\n",
    "        self.dim_entrada=dim_entrada[0]                                  # dimension de la capa de entrada\n",
    "        self.dim_oculta=50                                               # dimension de la capa oculta del perceptron (50 nodos, se puede cambiar)\n",
    "        self.lineal=torch.nn.Linear(self.dim_entrada, self.dim_oculta)   # entrada (transformaacion lineal)\n",
    "        self.salida=torch.nn.Linear(self.dim_oculta, dim_salida)         # salida\n",
    "        \n",
    "    def forward(self, x):  # metodo de creacion del perceptron\n",
    "        x=torch.from_numpy(x).float().to(self.hardware)\n",
    "        x=torch.nn.functional.relu(self.lineal(x)) # unidad rectificado lineal\n",
    "        x=self.salida(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clase Agente \n",
    "\n",
    "Se cambia la clase Agente para que los Q-valores sean obtenidos con el perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agente(object):\n",
    "    \n",
    "    def __init__(self, entorno):\n",
    "        self.obs_dim=entorno.observation_space.shape              # dimension espacio observacion\n",
    "        self.obs_sup=entorno.observation_space.high               # limite superior\n",
    "        self.obs_inf=entorno.observation_space.low                # limite inferior\n",
    "        self.obs_bins=NUM_BINS                                    # discretizacion de un espacio continuo, numero de bins\n",
    "        self.ancho_bin=(self.obs_sup-self.obs_inf)/self.obs_bins  # ancho de cada parte de la discretizacion\n",
    "        self.dim_accion=entorno.action_space.n                    # dimension espacio accion\n",
    "        \n",
    "        self.Q=Perceptron(self.obs_dim, self.dim_accion)                    # Q-valores\n",
    "        self.Q_optimizador=torch.optim.Adam(self.Q.parameters(), lr=1e-5)   # optimizador Adam\n",
    "        \n",
    "        self.alfa=ALFA        # tasa de aprendizaje           \n",
    "        self.gamma=GAMMA      # factor de descuento\n",
    "        self.epsilon=EPSILON  # prob para escoger accion \n",
    "        \n",
    "        \n",
    "    def discretiza(self, obs):  # discretiza un espacio continuo\n",
    "        return tuple(((obs-self.obs_inf)/self.ancho_bin).astype(int))  # binning\n",
    "        \n",
    "        \n",
    "    def accion(self, obs):   # realiza la accion del agente\n",
    "        obs_discreta=self.discretiza(obs) # discretiza espacio\n",
    "        \n",
    "        # politica epsilon-greedy para escoger la accion (mejor probabilidad 1-eps, peor es epsilon)\n",
    "        if self.epsilon>MIN_EPSILON: self.epsilon-=DECAY_EPSILON\n",
    "            \n",
    "            \n",
    "        if np.random.random()>self.epsilon: \n",
    "            return np.argmax(self.Q(obs_discreta).data.to(torch.device('cpu')).numpy())\n",
    "        else: \n",
    "            return np.random.choice([a for a in range(self.dim_accion)])\n",
    "        \n",
    "        \n",
    "    def aprende(self, obs, acciona, recompensa, siguiente_obs): # metodo de aprendizaje\n",
    "        td_objetivo=recompensa+self.gamma*torch.max(self.Q(siguiente_obs))         # valor de aprendizaje\n",
    "        td_error=torch.nn.functional.mse_loss(self.Q(obs)[acciona], td_objetivo)   # error aprendizaje\n",
    "        self.Q_optimizador.zero_grad()\n",
    "        td_error.backward()\n",
    "        self.Q_optimizador.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episodio #1 finalizado en 14 pasos. Recompensa total:14.0\n",
      "\n",
      "Episodio #2 finalizado en 15 pasos. Recompensa total:15.0\n",
      "\n",
      "Episodio #3 finalizado en 17 pasos. Recompensa total:17.0\n",
      "\n",
      "Episodio #4 finalizado en 20 pasos. Recompensa total:20.0\n",
      "\n",
      "Episodio #5 finalizado en 21 pasos. Recompensa total:21.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "entorno=gym.make('CartPole-v0')\n",
    "for episodio in range(5):  # 5 episodios\n",
    "    \n",
    "    done=False\n",
    "    \n",
    "    observacion=entorno.reset()  # observacion\n",
    "    \n",
    "    recompensa_total=0.          # recompensa total en cada episodio\n",
    "    \n",
    "    paso=0                       # paso en cada episodio\n",
    "     \n",
    "    while not done:\n",
    "        entorno.render()  # muestra el entorno\n",
    "        \n",
    "        accion=entorno.action_space.sample() # accion aleatoria, se cambiara por el agente\n",
    "        \n",
    "        siguiente_estado, recompensa, done, info=entorno.step(accion)\n",
    "        \n",
    "        recompensa_total+=recompensa\n",
    "        \n",
    "        paso+=1\n",
    "        \n",
    "        observacion=siguiente_estado\n",
    "        \n",
    "    print ('\\nEpisodio #{} finalizado en {} pasos. Recompensa total:{}'.format(episodio+1, paso, recompensa_total))\n",
    "    \n",
    "entorno.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
