{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mountain Car"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Descripcion \n",
    "\n",
    "\n",
    "+ **Espacio de observacion (Box(2,))**\n",
    "\n",
    "+ Posicion: [-1.2, 0.6]\n",
    "+ Velocidad: [-0.07, 0.07]\n",
    "\n",
    "\n",
    "+ **Espacio de accion (Discrete(3))**\n",
    "\n",
    "+ Izquierda: 0\n",
    "+ No hacer nada: 1\n",
    "+ Derecha: 2\n",
    "\n",
    "\n",
    "El objetivo es llevar el coche hasta la posicion 0.5\n",
    "\n",
    "Para cada paso temporal que el coche no este en esa posicion, la recompensa sera -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# primero test aleatorio del entorno\n",
    "\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "entorno=gym.make('MountainCar-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episodio #1 finalizado en 200 pasos. Recompensa total:-200.0\n",
      "\n",
      "Episodio #2 finalizado en 200 pasos. Recompensa total:-200.0\n",
      "\n",
      "Episodio #3 finalizado en 200 pasos. Recompensa total:-200.0\n",
      "\n",
      "Episodio #4 finalizado en 200 pasos. Recompensa total:-200.0\n",
      "\n",
      "Episodio #5 finalizado en 200 pasos. Recompensa total:-200.0\n"
     ]
    }
   ],
   "source": [
    "for episodio in range(5):  # 5 episodios\n",
    "    \n",
    "    done=False\n",
    "    \n",
    "    observacion=entorno.reset()  # observacion\n",
    "    \n",
    "    recompensa_total=0.          # recompensa total en cada episodio\n",
    "    \n",
    "    paso=0                       # paso en cada episodio\n",
    "     \n",
    "    while not done:\n",
    "        entorno.render()  # muestra el entorno\n",
    "        \n",
    "        accion=entorno.action_space.sample() # accion aleatoria, se cambiara por el agente\n",
    "        \n",
    "        siguiente_estado, recompensa, done, info=entorno.step(accion)\n",
    "        \n",
    "        recompensa_total+=recompensa\n",
    "        \n",
    "        paso+=1\n",
    "        \n",
    "        observacion=siguiente_estado\n",
    "        \n",
    "    print ('\\nEpisodio #{} finalizado en {} pasos. Recompensa total:{}'.format(episodio+1, paso, recompensa_total))\n",
    "    \n",
    "entorno.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementacion del agente\n",
    "\n",
    "Ahora vamos a crear un agente basado en el algoritmo Q-learning (Q viene de quality, la calidad de la accion tomada en un estado dado). Dicho algoritmo tiene la ecuacion:\n",
    "\n",
    "$$Q_{nueva}(e_{t}, a_{t})=(1-\\alpha)路Q(e_{t}, a_{t}) + \\alpha路[r_{t} + \\gamma路\\max_{a}Q(e_{t+1}, a_{t+1})]$$\n",
    "\n",
    "\n",
    "donde:\n",
    "+ $e_{t}$ es el estado en el tiempo t\n",
    "+ $a_{t}$ es la accion en el tiempo t\n",
    "+ $\\alpha$ es la tasa de aprendizaje $(0<\\alpha \\leq{1})$\n",
    "+ $Q(e_{t}, a_{t})$ es el viejo valor de calidad\n",
    "+ $[r_{t} + \\gamma路\\max_{a}Q(e_{t+1}, a_{t+1})]$ es el valor aprendido\n",
    "+ $r_{t}$ es la recompensa recibida al pasar del estado $e_{t}$ al estado $e_{t+1}$\n",
    "+ $\\gamma$ es el factor de descuento $(0\\leq \\gamma \\leq 1)$. Evalua las recompensas recibidas anteriormente con un valor mayor que las recibidas posteriormente, se puede interpretar como la probabilidad de tener exito (o sobrevivir) en cada paso temporal+ $\\max_{a}Q(e_{t+1}, a_{t+1})$ es la estimacion del valor optimo futuro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se crea la clase agente (basada en numpy)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class Agente(object):\n",
    "    \n",
    "    def __init__(self, entorno):\n",
    "        self.obs_dim=entorno.observation_space.shape              # dimension espacio observacion\n",
    "        self.obs_sup=entorno.observation_space.high               # limite superior\n",
    "        self.obs_inf=entorno.observation_space.low                # limite inferior\n",
    "        self.obs_bins=NUM_BINS                                    # discretizacion de un espacio continuo, numero de bins\n",
    "        self.ancho_bin=(self.obs_sup-self.obs_inf)/self.obs_bins  # ancho de cada parte de la discretizacion\n",
    "        self.dim_accion=entorno.action_space.n                    # dimension espacio accion\n",
    "        \n",
    "        self.Q=np.zeros((self.obs_bins+1, \n",
    "                         self.obs_bins+1,\n",
    "                         self.dim_accion))  # array para guardar los Q-valores\n",
    "        \n",
    "        self.alfa=ALFA        # tasa de aprendizaje           \n",
    "        self.gamma=GAMMA      # factor de descuento\n",
    "        self.epsilon=EPSILON  # prob para escoger accion \n",
    "        \n",
    "        \n",
    "    def discretiza(self, obs):  # discretiza un espacio continuo\n",
    "        return tuple(((obs-self.obs_inf)/self.ancho_bin).astype(int))  # binning\n",
    "        \n",
    "        \n",
    "    def accion(self, obs):   # realiza la accion del agente\n",
    "        obs_discreta=self.discretiza(obs) # discretiza espacio\n",
    "        \n",
    "        # politica epsilon-greedy para escoger la accion (mejor probabilidad 1-eps, peor es epsilon)\n",
    "        if self.epsilon>MIN_EPSILON: self.epsilon-=DECAY_EPSILON\n",
    "            \n",
    "        if np.random.random()>self.epsilon: return np.argmax(self.Q[obs_discreta])\n",
    "        else: return np.random.choice([a for a in range(self.dim_accion)])\n",
    "        \n",
    "        \n",
    "    def aprende(self, obs, acciona, recompensa, siguiente_obs): # metodo de aprendizaje\n",
    "        obs_discreta=self.discretiza(obs)                       # discretiza las observaciones\n",
    "        siguiente_obs_discreta=self.discretiza(siguiente_obs)\n",
    "        \n",
    "        td_objetivo=recompensa+self.gamma*np.max(self.Q[siguiente_obs_discreta]) # valor de aprendizaje\n",
    "        td_error=td_objetivo-self.Q[obs_discreta][acciona]                       # error aprendizaje\n",
    "        self.Q[obs_discreta][acciona]+=self.alfa*td_error                        # actualizacion Q-valores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fase entrenamiento-testeo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# primero las constantes (se pueden considerar hiperparametros)\n",
    "\n",
    "MIN_EPSILON=0.005                               # epsilon minimo, probabilidad minima para escoger una accion\n",
    "EPSILON=1.                                      # prob para escoger accion\n",
    "NUM_MAX_EPIS=50000                              # numero maximo de episodios\n",
    "PASOS_POR_EPI=200                               # pasos por episodio\n",
    "NUM_MAX_PASOS=NUM_MAX_EPIS*PASOS_POR_EPI        # numero maximo de pasos por episodio  \n",
    "DECAY_EPSILON=500*MIN_EPSILON/NUM_MAX_PASOS     # ajuste del valor de epsilon \n",
    "ALFA=0.05                                       # tasa de aprendizaje\n",
    "GAMMA=0.98                                      # factor de descuento\n",
    "NUM_BINS=20                                     # numero de binnings para discretizacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funcion de entrenamiento\n",
    "\n",
    "def train(agente, entorno):   \n",
    "    mejor_recompensa=-float('inf')  # se inicializa la mejor recompensa (al minimo posible)\n",
    "    \n",
    "    for episodio in range(NUM_MAX_EPIS):\n",
    "        done=False\n",
    "        obs=entorno.reset()\n",
    "        recompensa_total=0.\n",
    "        \n",
    "        while not done:\n",
    "            accion=agente.accion(obs)\n",
    "            siguiente_obs, recompensa, done, info=entorno.step(accion)\n",
    "            agente.aprende(obs, accion, recompensa, siguiente_obs)\n",
    "            obs=siguiente_obs\n",
    "            recompensa_total+=recompensa\n",
    "            \n",
    "        if recompensa_total>mejor_recompensa: mejor_recompensa=recompensa_total\n",
    "        if episodio%2000==0:\n",
    "            print ('Episodio:{}..|..Recompensa:{}, Mejor Recompensa:{}..|..Epsilon:{}'.format(episodio, recompensa_total, mejor_recompensa, agente.epsilon))\n",
    "    \n",
    "    return np.argmax(agente.Q, axis=2)   # devuelve la politica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funcion de testeo\n",
    "\n",
    "def test(agente, entorno, politica):\n",
    "    done=False\n",
    "    obs=entorno.reset()\n",
    "    recompensa_total=0.\n",
    "    \n",
    "    while not done:\n",
    "        accion=politica[agente.discretiza(obs)]\n",
    "        siguiente_obs, recompensa, done, info=entorno.step(accion)\n",
    "        obs=siguiente_obs\n",
    "        recompensa_total+=recompensa\n",
    "        \n",
    "    return recompensa_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio:0..|..Recompensa:-200.0, Mejor Recompensa:-200.0..|..Epsilon:0.999949999999993\n",
      "Episodio:2000..|..Recompensa:-200.0, Mejor Recompensa:-200.0..|..Epsilon:0.8999499999860152\n",
      "Episodio:4000..|..Recompensa:-200.0, Mejor Recompensa:-200.0..|..Epsilon:0.7999499999720374\n",
      "Episodio:6000..|..Recompensa:-200.0, Mejor Recompensa:-200.0..|..Epsilon:0.6999499999580596\n",
      "Episodio:8000..|..Recompensa:-200.0, Mejor Recompensa:-168.0..|..Epsilon:0.5999579999440829\n",
      "Episodio:10000..|..Recompensa:-200.0, Mejor Recompensa:-161.0..|..Epsilon:0.5000474999301177\n",
      "Episodio:12000..|..Recompensa:-200.0, Mejor Recompensa:-153.0..|..Epsilon:0.40107799993824905\n",
      "Episodio:14000..|..Recompensa:-200.0, Mejor Recompensa:-125.0..|..Epsilon:0.3046594999461811\n",
      "Episodio:16000..|..Recompensa:-200.0, Mejor Recompensa:-118.0..|..Epsilon:0.2136659999496329\n",
      "Episodio:18000..|..Recompensa:-157.0, Mejor Recompensa:-115.0..|..Epsilon:0.12848024994718332\n",
      "Episodio:20000..|..Recompensa:-145.0, Mejor Recompensa:-109.0..|..Epsilon:0.04869849994874167\n",
      "Episodio:22000..|..Recompensa:-187.0, Mejor Recompensa:-91.0..|..Epsilon:0.0049999999486979654\n",
      "Episodio:24000..|..Recompensa:-143.0, Mejor Recompensa:-91.0..|..Epsilon:0.0049999999486979654\n",
      "Episodio:26000..|..Recompensa:-180.0, Mejor Recompensa:-91.0..|..Epsilon:0.0049999999486979654\n",
      "Episodio:28000..|..Recompensa:-146.0, Mejor Recompensa:-91.0..|..Epsilon:0.0049999999486979654\n",
      "Episodio:30000..|..Recompensa:-150.0, Mejor Recompensa:-86.0..|..Epsilon:0.0049999999486979654\n",
      "Episodio:32000..|..Recompensa:-142.0, Mejor Recompensa:-86.0..|..Epsilon:0.0049999999486979654\n",
      "Episodio:34000..|..Recompensa:-120.0, Mejor Recompensa:-86.0..|..Epsilon:0.0049999999486979654\n",
      "Episodio:36000..|..Recompensa:-121.0, Mejor Recompensa:-86.0..|..Epsilon:0.0049999999486979654\n",
      "Episodio:38000..|..Recompensa:-137.0, Mejor Recompensa:-86.0..|..Epsilon:0.0049999999486979654\n",
      "Episodio:40000..|..Recompensa:-177.0, Mejor Recompensa:-86.0..|..Epsilon:0.0049999999486979654\n",
      "Episodio:42000..|..Recompensa:-121.0, Mejor Recompensa:-86.0..|..Epsilon:0.0049999999486979654\n",
      "Episodio:44000..|..Recompensa:-121.0, Mejor Recompensa:-86.0..|..Epsilon:0.0049999999486979654\n",
      "Episodio:46000..|..Recompensa:-188.0, Mejor Recompensa:-86.0..|..Epsilon:0.0049999999486979654\n",
      "Episodio:48000..|..Recompensa:-139.0, Mejor Recompensa:-86.0..|..Epsilon:0.0049999999486979654\n"
     ]
    }
   ],
   "source": [
    "# se ejecuta todo\n",
    "\n",
    "if __name__=='__main__':\n",
    "    \n",
    "    entorno=gym.make('MountainCar-v0')\n",
    "    agente=Agente(entorno)\n",
    "    politica=train(agente, entorno)\n",
    "    \n",
    "    gym_monitor='./gym_monitor'  # para guardar video\n",
    "    entorno=gym.wrappers.Monitor(entorno, gym_monitor, force=True)\n",
    "    \n",
    "    for i in range(500):\n",
    "        test(agente, entorno, politica)\n",
    "        \n",
    "    entorno.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
