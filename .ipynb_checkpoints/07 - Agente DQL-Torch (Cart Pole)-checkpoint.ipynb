{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cart Pole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Descripcion \n",
    "\n",
    "\n",
    "+ **Espacio de observacion (Box(4,))**\n",
    "\n",
    "+ Posicion del carro: [-2.4, 2.4]\n",
    "+ Velocidad del carro: [$-\\infty$, $\\infty$]\n",
    "+ Angulo del palo: [-41.8, 41.8]\n",
    "+ Velocidad del palo en la punta: [$-\\infty$, $\\infty$]\n",
    "\n",
    "\n",
    "+ **Espacio de accion (Discrete(2))**\n",
    "\n",
    "+ Izquierda: 0\n",
    "+ Derecha: 1\n",
    "\n",
    "\n",
    "El objetivo es mantener el palo vertical moviendo a izquierda y derecha el carro.\n",
    "\n",
    "\n",
    "La recompensa es +1 para cada paso temporal. El episodio se termina si el angulo es mayor a $\\pm 12$ grados o si el carro sobrepasa la posicion $\\pm 2.4$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementaremos un perceptron (red neuronal) para calcular los Q-valores, en vez de usar el algoritmo Q-learning usado anteriormente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perceptron implementado en Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Perceptron(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, dim_entrada, dim_salida, hardware=torch.device('cpu')):\n",
    "        super(Perceptron, self).__init__()                               # herencia\n",
    "        self.hardware=hardware                                           # hardware usado por el perceptron\n",
    "        self.dim_entrada=dim_entrada[0]                                  # dimension de la capa de entrada\n",
    "        self.dim_oculta=50                                               # dimension de la capa oculta del perceptron (50 nodos, se puede cambiar)\n",
    "        self.lineal=torch.nn.Linear(self.dim_entrada, self.dim_oculta)   # entrada (transformaacion lineal)\n",
    "        self.salida=torch.nn.Linear(self.dim_oculta, dim_salida)         # salida\n",
    "        \n",
    "    def forward(self, x):  # metodo de creacion del perceptron\n",
    "        x=torch.from_numpy(np.array(x)).float().to(self.hardware)\n",
    "        x=torch.nn.functional.relu(self.lineal(x)) # unidad rectificado lineal\n",
    "        x=self.salida(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clase Agente \n",
    "\n",
    "Se cambia la clase Agente para que los Q-valores sean obtenidos con el perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agente(object):\n",
    "    \n",
    "    def __init__(self, entorno):\n",
    "        self.obs_dim=entorno.observation_space.shape              # dimension espacio observacion\n",
    "        self.obs_sup=entorno.observation_space.high               # limite superior\n",
    "        self.obs_inf=entorno.observation_space.low                # limite inferior\n",
    "        self.obs_bins=NUM_BINS                                    # discretizacion de un espacio continuo, numero de bins\n",
    "        self.ancho_bin=(self.obs_sup-self.obs_inf)/self.obs_bins  # ancho de cada parte de la discretizacion\n",
    "        self.dim_accion=entorno.action_space.n                    # dimension espacio accion\n",
    "        \n",
    "        self.Q=Perceptron(self.obs_dim, self.dim_accion)                    # Q-valores\n",
    "        self.Q_optimizador=torch.optim.Adam(self.Q.parameters(), lr=1e-5)   # optimizador Adam\n",
    "        \n",
    "        self.alfa=ALFA        # tasa de aprendizaje           \n",
    "        self.gamma=GAMMA      # factor de descuento\n",
    "        self.epsilon=EPSILON  # prob para escoger accion \n",
    "        \n",
    "        \n",
    "    def discretiza(self, obs):  # discretiza un espacio continuo\n",
    "        return tuple(((obs-self.obs_inf)/self.ancho_bin).astype(int))  # binning\n",
    "        \n",
    "        \n",
    "    def accion(self, obs):   # realiza la accion del agente\n",
    "        obs_discreta=self.discretiza(obs) # discretiza espacio\n",
    "        \n",
    "        # politica epsilon-greedy para escoger la accion (mejor probabilidad 1-eps, peor es epsilon)\n",
    "        if self.epsilon>MIN_EPSILON: self.epsilon-=DECAY_EPSILON\n",
    "            \n",
    "            \n",
    "        if np.random.random()>self.epsilon: \n",
    "            return np.argmax(self.Q(obs_discreta).data.to(torch.device('cpu')).numpy())\n",
    "        else: \n",
    "            return np.random.choice([a for a in range(self.dim_accion)])\n",
    "        \n",
    "        \n",
    "    def aprende(self, obs, acciona, recompensa, siguiente_obs): # metodo de aprendizaje\n",
    "        td_objetivo=recompensa+self.gamma*torch.max(self.Q(siguiente_obs))         # valor de aprendizaje\n",
    "        td_error=torch.nn.functional.mse_loss(self.Q(obs)[acciona], td_objetivo)   # error aprendizaje\n",
    "        self.Q_optimizador.zero_grad()\n",
    "        td_error.backward()\n",
    "        self.Q_optimizador.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejecucion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# primero las constantes (se pueden considerar hiperparametros)\n",
    "\n",
    "MIN_EPSILON=0.005                               # epsilon minimo, probabilidad minima para escoger una accion\n",
    "EPSILON=1.                                      # prob para escoger accion\n",
    "NUM_MAX_EPIS=50000                              # numero maximo de episodios\n",
    "PASOS_POR_EPI=200                               # pasos por episodio\n",
    "NUM_MAX_PASOS=NUM_MAX_EPIS*PASOS_POR_EPI        # numero maximo de pasos por episodio  \n",
    "DECAY_EPSILON=500*MIN_EPSILON/NUM_MAX_PASOS     # ajuste del valor de epsilon \n",
    "ALFA=0.05                                       # tasa de aprendizaje\n",
    "GAMMA=0.98                                      # factor de descuento\n",
    "NUM_BINS=20                                     # numero de binnings para discretizacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 0 acabado en 16 pasos..|..Recompensa:16.0, Recompensa media:16.0, Mejor Recompensa:16.0\n",
      "Episodio 2000 acabado en 14 pasos..|..Recompensa:14.0, Recompensa media:22.330834582708647, Mejor Recompensa:81.0\n",
      "Episodio 4000 acabado en 67 pasos..|..Recompensa:67.0, Recompensa media:22.331667083229192, Mejor Recompensa:102.0\n",
      "Episodio 6000 acabado en 20 pasos..|..Recompensa:20.0, Recompensa media:22.318280286618897, Mejor Recompensa:118.0\n",
      "Episodio 8000 acabado en 13 pasos..|..Recompensa:13.0, Recompensa media:22.340832395950507, Mejor Recompensa:118.0\n",
      "Episodio 10000 acabado en 13 pasos..|..Recompensa:13.0, Recompensa media:22.242975702429757, Mejor Recompensa:118.0\n",
      "Episodio 12000 acabado en 25 pasos..|..Recompensa:25.0, Recompensa media:22.254728772602284, Mejor Recompensa:118.0\n",
      "Episodio 14000 acabado en 14 pasos..|..Recompensa:14.0, Recompensa media:22.220341404185415, Mejor Recompensa:143.0\n",
      "Episodio 16000 acabado en 41 pasos..|..Recompensa:41.0, Recompensa media:22.194925317167677, Mejor Recompensa:143.0\n",
      "Episodio 18000 acabado en 26 pasos..|..Recompensa:26.0, Recompensa media:22.13304816399089, Mejor Recompensa:143.0\n",
      "Episodio 20000 acabado en 19 pasos..|..Recompensa:19.0, Recompensa media:22.071246437678116, Mejor Recompensa:143.0\n",
      "Episodio 22000 acabado en 19 pasos..|..Recompensa:19.0, Recompensa media:21.98581882641698, Mejor Recompensa:143.0\n"
     ]
    }
   ],
   "source": [
    "# se ejecuta todo\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "if __name__=='__main__':\n",
    "    \n",
    "    entorno=gym.make('CartPole-v0')\n",
    "    \n",
    "    agente=Agente(entorno)\n",
    "    primer_episodio=True\n",
    "    recompensas_episodio=[]\n",
    "    \n",
    "    for episodio in range(NUM_MAX_EPIS):\n",
    "        observacion=entorno.reset()\n",
    "        recompensa_total=0.\n",
    "        \n",
    "        for paso in range(NUM_MAX_PASOS):\n",
    "            #entorno.render()\n",
    "            accion=agente.accion(observacion)\n",
    "            \n",
    "            siguiente_obs, recompensa, done, info=entorno.step(accion)\n",
    "            agente.aprende(observacion, accion, recompensa, siguiente_obs)\n",
    "            \n",
    "            observacion=siguiente_obs\n",
    "            recompensa_total+=recompensa\n",
    "            \n",
    "            if done:\n",
    "                if primer_episodio:\n",
    "                    max_recompensa=recompensa_total\n",
    "                    primer_episodio=False\n",
    "                recompensas_episodio.append(recompensa_total)\n",
    "                \n",
    "                if recompensa_total>max_recompensa:\n",
    "                    max_recompensa=recompensa_total\n",
    "                \n",
    "                if episodio%2000==0:\n",
    "                    print ('Episodio {} acabado en {} pasos..|..Recompensa:{}, Recompensa media:{}, Mejor Recompensa:{}'.format(episodio, paso+1, recompensa_total, np.mean(recompensas_episodio),max_recompensa))\n",
    "                \n",
    "                break\n",
    "        entorno.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
