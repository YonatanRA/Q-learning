{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mountain Car"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Descripcion \n",
    "\n",
    "\n",
    "+ **Espacio de observacion (Box(2,))**\n",
    "\n",
    "+ Posicion: [-1.2, 0.6]\n",
    "+ Velocidad: [-0.07, 0.07]\n",
    "\n",
    "\n",
    "+ **Espacio de accion (Discrete(3))**\n",
    "\n",
    "+ Izquierda: 0\n",
    "+ No hacer nada: 1\n",
    "+ Derecha: 2\n",
    "\n",
    "\n",
    "El objetivo es llevar el coche hasta la posicion 0.5\n",
    "\n",
    "Para cada paso temporal que el coche no este en esa posicion, la recompensa sera -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# primero test aleatorio del entorno\n",
    "\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "entorno=gym.make('MountainCar-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episodio #1 finalizado en 200 pasos. Recompensa total:-200.0\n",
      "\n",
      "Episodio #2 finalizado en 200 pasos. Recompensa total:-200.0\n",
      "\n",
      "Episodio #3 finalizado en 200 pasos. Recompensa total:-200.0\n",
      "\n",
      "Episodio #4 finalizado en 200 pasos. Recompensa total:-200.0\n",
      "\n",
      "Episodio #5 finalizado en 200 pasos. Recompensa total:-200.0\n",
      "\n",
      "Episodio #6 finalizado en 200 pasos. Recompensa total:-200.0\n",
      "\n",
      "Episodio #7 finalizado en 200 pasos. Recompensa total:-200.0\n",
      "\n",
      "Episodio #8 finalizado en 200 pasos. Recompensa total:-200.0\n",
      "\n",
      "Episodio #9 finalizado en 200 pasos. Recompensa total:-200.0\n",
      "\n",
      "Episodio #10 finalizado en 200 pasos. Recompensa total:-200.0\n"
     ]
    }
   ],
   "source": [
    "for episodio in range(10):  # 10 episodios\n",
    "    \n",
    "    done=False\n",
    "    \n",
    "    observacion=entorno.reset()  # observacion\n",
    "    \n",
    "    recompensa_total=0.          # recompensa total en cada episodio\n",
    "    \n",
    "    paso=0                       # paso en cada episodio\n",
    "     \n",
    "    while not done:\n",
    "        entorno.render()  # muestra el entorno\n",
    "        \n",
    "        accion=entorno.action_space.sample() # accion aleatoria, se cambiara por el agente\n",
    "        \n",
    "        siguiente_estado, recompensa, done, info=entorno.step(accion)\n",
    "        \n",
    "        recompensa_total+=recompensa\n",
    "        \n",
    "        paso+=1\n",
    "        \n",
    "        observacion=siguiente_estado\n",
    "        \n",
    "    print ('\\nEpisodio #{} finalizado en {} pasos. Recompensa total:{}'.format(episodio+1, paso, recompensa_total))\n",
    "    \n",
    "entorno.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementacion del agente\n",
    "\n",
    "Ahora vamos a crear un agente basado en el algoritmo Q-learning (Q viene de quality, la calidad de la accion tomada en un estado dado). Dicho algoritmo tiene la ecuacion:\n",
    "\n",
    "$$Q_{nueva}(e_{t}, a_{t})=(1-\\alpha)路Q(e_{t}, a_{t}) + \\alpha路[r_{t} + \\gamma路\\max_{a}Q(e_{t+1}, a_{t+1})]$$\n",
    "\n",
    "\n",
    "donde:\n",
    "+ $e_{t}$ es el estado en el tiempo t\n",
    "+ $a_{t}$ es la accion en el tiempo t\n",
    "+ $\\alpha$ es la tasa de aprendizaje $(0<\\alpha \\leq{1})$\n",
    "+ $Q(e_{t}, a_{t})$ es el viejo valor de calidad\n",
    "+ $[r_{t} + \\gamma路\\max_{a}Q(e_{t+1}, a_{t+1})]$ es el valor aprendido\n",
    "+ $r_{t}$ es la recompensa recibida al pasar del estado $e_{t}$ al estado $e_{t+1}$\n",
    "+ $\\gamma$ es el factor de descuento $(0\\leq \\gamma \\leq 1)$. Evalua las recompensas recibidas anteriormente con un valor mayor que las recibidas posteriormente, se puede interpretar como la probabilidad de tener exito (o sobrevivir) en cada paso temporal\n",
    "+ $\\max_{a}Q(e_{t+1}, a_{t+1})$ es la estimacion del vaalor optimo futuro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se crea la clase agente (basada en numpy)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class Agente(object):\n",
    "    \n",
    "    def __init__(self, entorno):\n",
    "        self.obs_dim=entorno.observation_space.shape              # dimension espacio observacion\n",
    "        self.obs_sup=entorno.observation_space.high               # limite superior\n",
    "        self.obs_inf=entorno.observation_space.low                # limite inferior\n",
    "        self.obs_bins=NUM_BINS                                    # discretizacion de un espacio continuo, numero de bins\n",
    "        self.ancho_bin=(self.obs_sup-self.obs_inf)/self.obs_bins  # ancho de cada parte de la discretizacion\n",
    "        self.dim_accion=entorno.action_space.n                    # dimension espacio accion\n",
    "        self.Q=np.zeros((self.obs_bins+1, \n",
    "                         self.obs_bins+1,\n",
    "                         self.dim_accion))  # array para guardar los Q-valores\n",
    "        \n",
    "        self.alfa=ALFA        # tasa de aprendizaje           \n",
    "        self.gamma=GAMMA      # factor de descuento\n",
    "        self.epsilon=EPSILON  # \n",
    "        \n",
    "        \n",
    "    def discretiza(self, obs):  # discretiza un espacio continuo\n",
    "        return tuple(((self.obs_sup-self.obs_inf)/self.obs_bins).astype(int))  # binning\n",
    "        \n",
    "        \n",
    "    def accion(self, obs):   # realiza la accion del agente\n",
    "        obs_discreta=self.discretiza(obs) # discretiza espacio\n",
    "        \n",
    "        # politica epsilon-greedy para escoger la accion (mejor probabilidad 1-eps, peor es epsilon)\n",
    "        if self.epsilon>MIN_EPSILON: self.epsilon-=DECAY_EPSILON\n",
    "            \n",
    "        if np.random.random()>self.epsilon: return np.argmax(self.Q[obs_discreta])\n",
    "        else: return np.random.choice([a for a in range(self.dim_accion)])\n",
    "        \n",
    "        \n",
    "    def aprende(self, obs, acciona, recompensa, siguiente_obs): # metodo de aprendizaje\n",
    "        obs_discreta=self.discretiza(obs)    # discretiza las observaciones\n",
    "        siguiente_obs_discreta=self.discretiza(siguiente_obs)\n",
    "        \n",
    "        td_objetivo=recompensa+self.gamma*np.argmax(self.Q[siguiente_obs_discreta]) # valor de aprendizaje\n",
    "        td_error=td_objetivo-self.Q[obs_discreta][acciona]   # error aprendizaje\n",
    "        self.Q[obs_discreta][acciona]+=self.alfa*td_error    # actualizacion Q-valores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
