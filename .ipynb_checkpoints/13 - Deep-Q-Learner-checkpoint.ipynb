{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agente con CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "from UtilsDQL import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Deep_Q_Learner(object):\n",
    "    \n",
    "    def __init__(self, dim_estado, dim_accion, params):\n",
    "        \"\"\"\n",
    "        self.Q is the Action-Value function. This agent represents Q using a Neural Network\n",
    "        If the input is a single dimensional vector, uses a Single-Layer-Perceptron else if the input is 3 dimensional\n",
    "        image, use a Convolutional-Neural-Network\n",
    "\n",
    "        :param state_shape: Shape (tuple) of the observation/state\n",
    "        :param action_shape: Shape (number) of the discrete action space\n",
    "        :param params: A dictionary containing various Agent configuration parameters and hyper-parameters\n",
    "        \"\"\"\n",
    "        self.dim_estado=dim_estado\n",
    "        self.dim_accion=dim_accion\n",
    "        self.params=params\n",
    "        self.gamma=self.params['gamma']             # Agent's discount factor\n",
    "        self.tasa_apredizaje=self.params['lr']      # Agent's Q-learning rate\n",
    "        self.mejor_recompensa_media=-float('inf')   # Agent's personal best mean episode reward\n",
    "        self.mejor_recompensa=-float('inf')\n",
    "        self.pasos_entrenamiento=0                  # Number of training batch steps completed so far\n",
    "\n",
    "        if len(self.dim_estado)==1:                 # Single dimensional observation/state space\n",
    "            self.DQN=Perceptron\n",
    "        elif len(self.dim_estado)==3:               # 3D/image observation/state\n",
    "            self.DQN=CNN\n",
    "\n",
    "        self.Q=self.DQN(dim_estado, dim_accion, device).to(device)\n",
    "        self.Q.apply(xavier)\n",
    "\n",
    "        self.Q_optimizador=torch.optim.Adam(self.Q.parameters(), lr=self.tasa_apredizaje)\n",
    "        \n",
    "        if self.params['use_target_network']:\n",
    "            self.Q_objetivo=self.DQN(dim_estado, dim_accion, device).to(device)\n",
    "        # self.policy is the policy followed by the agent. This agents follows\n",
    "        # an epsilon-greedy policy w.r.t it's Q estimate.\n",
    "        self.politica=self.epsilon_greedy_Q\n",
    "        self.epsilon_max=params['epsilon_max']\n",
    "        self.epsilon_min=params['epsilon_min']\n",
    "        self.epsilon_decay=DecayLineal(valor_ini=self.epsilon_max,\n",
    "                                       valor_final=self.epsilon_min,\n",
    "                                       pasos_max=self.params['epsilon_decay_final_step'])\n",
    "        self.num_pasos=0\n",
    "\n",
    "        self.memoria=Memoria(capacidad=int(self.params['experience_memory_capacity']))  # Initialize an Experience memory with 1M capacity\n",
    "\n",
    "    def accion(self, observacion):\n",
    "        observacion=np.array(observacion)  # Observations could be lazy frames. So force fetch before moving forward\n",
    "        observacion=observacion/255.       # Scale/Divide by max limit of obs' dtype. 255 for uint8\n",
    "        if len(observacion.shape)==3:      # Single image (not a batch)\n",
    "            if observacion.shape[2]<observacion.shape[0]:  # Probably observation is in W x H x C format\n",
    "                # NOTE: This is just an additional check. The env wrappers are taking care of this conversion already\n",
    "                # Reshape to C x H x W format as per PyTorch's convention\n",
    "                observacion=observacion.reshape(observacion.shape[2], observacion.shape[1], observacion.shape[0])\n",
    "            observacion=np.expand_dims(observacion, 0)  # Create a batch dimension\n",
    "        return self.politica(observacion)\n",
    "\n",
    "    def epsilon_greedy_Q(self, observacion):\n",
    "        # Decay Epsilon/exploration as per schedule\n",
    "        writer.add_scalar('DQL/epsilon', self.epsilon_decay(self.num_pasos), self.num_pasos)\n",
    "        self.num_pasos+=1\n",
    "        if random.random()<self.epsilon_decay(self.num_pasos) and not self.params['test']:\n",
    "            accion=random.choice([i for i in range(self.dim_accion)])\n",
    "        else:\n",
    "            accion=np.argmax(self.Q(observacion).data.to(torch.device('cpu')).numpy())\n",
    "        return accion\n",
    "\n",
    "    def aprende(self, s, a, r, s_next, done):\n",
    "        # TD(0) Q-learning\n",
    "        if done:  # End of episode\n",
    "            td_objetivo=recompensa+0.  # Set the value of terminal state to zero\n",
    "        else:\n",
    "            td_objetivo=r+self.gamma*torch.max(self.Q(s_next))\n",
    "        td_error=td_objetivo-self.Q(s)[a]\n",
    "        # Update Q estimate\n",
    "        #self.Q(s)[a]=self.Q(s)[a]+self.tasa_aprendizaje*td_error\n",
    "        self.Q_optimizador.zero_grad()\n",
    "        td_error.backward()\n",
    "        self.Q_optimizador.step()\n",
    "\n",
    "    def aprende_de_experiencia(self, experiencias):\n",
    "        batch_xp=Experiencia(*zip(*experiencias))\n",
    "        obs_batch=np.array(batch_xp.obs)/255.  # Scale/Divide by max limit of obs's dtype. 255 for uint8\n",
    "        accion_batch=np.array(batch_xp.action)\n",
    "        recompensa_batch=np.array(batch_xp.reward)\n",
    "        # Clip the rewards\n",
    "        if self.params['clip_rewards']:\n",
    "            recompensa_batch=np.sign(recompensa_batch)\n",
    "        next_obs_batch=np.array(batch_xp.next_obs)/255.  # Scale/Divide by max limit of obs' dtype. 255 for uint8\n",
    "        done_batch=np.array(batch_xp.done)\n",
    "\n",
    "        if self.params['use_target_network']:\n",
    "            #if self.training_steps_completed % self.params['target_network_update_freq'] == 0:\n",
    "            if self.num_pasos%self.params['target_network_update_freq']==0:\n",
    "                # The *update_freq is the Num steps after which target net is updated.\n",
    "                # A schedule can be used instead to vary the update freq.\n",
    "                self.Q_objetivo.load_state_dict(self.Q.state_dict())\n",
    "            td_objetivo=recompensa_batch+~done_batch* \\\n",
    "                np.tile(self.gamma, len(next_obs_batch))* \\\n",
    "                self.Q_objetivo(next_obs_batch).max(1)[0].data.cpu().numpy()\n",
    "        else:\n",
    "            td_objetivo=recompensa_batch+~done_batch* \\\n",
    "                np.tile(self.gamma, len(next_obs_batch))* \\\n",
    "                self.Q(next_obs_batch).detach().max(1)[0].data.cpu().numpy()\n",
    "\n",
    "        td_objetivo=torch.from_numpy(td_objetivo).to(device)\n",
    "        accion_idx=torch.from_numpy(accion_batch).to(device)\n",
    "        td_error=torch.nn.functional.mse_loss(self.Q(obs_batch).gather(1, action_idx.view(-1, 1)),\n",
    "                                              td_objetivo.float().unsqueeze(1))\n",
    "\n",
    "        self.Q_optimizador.zero_grad()\n",
    "        td_error.mean().backward()\n",
    "        writer.add_scalar('DQL/td_error', td_error.mean(), self.num_pasos)\n",
    "        self.Q_optimizador.step()\n",
    "\n",
    "        \n",
    "    def replay_experiencia(self, batch_size=None):\n",
    "        batch_size=batch_size if batch_size is not None else self.params['replay_batch_size']\n",
    "        experiencia_batch=self.memoria.sample(batch_size)\n",
    "        self.aprende_de_experiencia(experiencia_batch)\n",
    "        self.pasos_entrenamiento+=1  # Increment the number of training batch steps complemented\n",
    "\n",
    "        \n",
    "    def guardar(self, nombre_ent):\n",
    "        archivo=self.params['save_dir']+'DQL_'+nombre_ent+'.ptm'\n",
    "        estado_agente={'Q': self.Q.state_dict(),\n",
    "                       'best_mean_reward': self.mejor_recompensa_media,\n",
    "                       'best_reward': self.recompensa_media};\n",
    "        torch.save(estado_agente, archivo)\n",
    "        print('Estado del agente guardado en ', archivo)\n",
    "\n",
    "        \n",
    "    def cargar(self, nombre_ent):\n",
    "        archivo=self.params['load_dir']+'DQL_'+nombre_ent+'.ptm'\n",
    "        estado_agente=torch.load(archivo, map_location= lambda x, loc: x)\n",
    "        \n",
    "        self.Q.load_state_dict(estado_agente['Q'])\n",
    "        self.Q.to(device)\n",
    "        self.mejor_recompensa_media=estado_agente['best_mean_reward']\n",
    "        self.recompensa_media=estado_agente['best_reward']\n",
    "        \n",
    "        print('Caragdo el estado del Q modelo desde', archivo,\n",
    "              ' con un mejor recompensa media de:', self.mejor_recompensa_media,\n",
    "              ' y una mejor recompensa de :', self.recompensa_media)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
