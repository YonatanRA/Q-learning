{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Herramientas para creacion de agente DQL con red convolucional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a crear algunas herramientas para un agente con una red neuronal convolucional que sea capaz de reconocer imagenes como datos de entrada. Entrenaremos al agente con juegos de Atari, por lo que tambien crearemos las herramientas del entorno.\n",
    "\n",
    "Recordemos primero el decay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import atari_py\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from gym.spaces.box import Box\n",
    "import cv2\n",
    "import random\n",
    "import torch\n",
    "import json\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecayLineal(object):\n",
    "    def __init__(self, valor_ini, valor_final, pasos_max):\n",
    "        assert valor_ini>valor_final, 'valor inicial > valor final'\n",
    "        self.valor_ini=valor_ini\n",
    "        self.valor_final=valor_final\n",
    "        self.decay=(valor_ini-valor_final)/pasos_max\n",
    "\n",
    "    def __call__(self, num_pasos):\n",
    "        valor_actual=self.valor_ini-self.decay*num_pasos\n",
    "        if valor_actual<self.valor_final:\n",
    "            valor_actual=self.valor_final\n",
    "        return valor_actual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seguiremos implementando la red convolucional con Torch de 3 capas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, dim_entrada, dim_salida, device='cpu'): # device=cpu o cuda\n",
    "        \n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        self.device=device\n",
    "        \n",
    "        self.capa1=torch.nn.Sequential(\n",
    "                    torch.nn.Conv2d(dim_entrad[0],\n",
    "                                    64,\n",
    "                                    kernel_size=4,\n",
    "                                    stride=2,\n",
    "                                    padding=1),\n",
    "                    torch.nn.ReLU())\n",
    "        \n",
    "        \n",
    "        self.capa2=torch.nn.Sequential(\n",
    "                    torch.nn.Conv2d(64,\n",
    "                                    32,\n",
    "                                    kernel_size=4,\n",
    "                                    stride=2,\n",
    "                                    padding=0),\n",
    "                    torch.nn.ReLU())\n",
    "        \n",
    "        \n",
    "        self.capa3=torch.nn.Sequential(\n",
    "                    torch.nn.Conv2d(32,\n",
    "                                    32,\n",
    "                                    kernel_size=3,\n",
    "                                    stride=2,\n",
    "                                    padding=0),\n",
    "                    torch.nn.ReLU())\n",
    "        \n",
    "        self.salida=torch.nn.Linear(18*18*32, dim_salida)\n",
    "        \n",
    "       \n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x=torch.from_numpy(x).float().to(self.device)\n",
    "        x=self.capa1(x)\n",
    "        x=self.capa2(x)\n",
    "        x=self.capa3(x)\n",
    "        x=x.view(x.shape[0], -1)\n",
    "        x=self.salida(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podriamos a単adir mas capas a la red.\n",
    "\n",
    "Ahora que hemos creado la red convolucional, vamos a crear un controlador de hiperparametros a traves de un JSON, siendo de esta manera mas facil manejar la red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParamControl(object):\n",
    "    \n",
    "    def __init__(self, archivo): # archivo json, path\n",
    "        \n",
    "        self.params=json.load(open(archivo, 'r'))\n",
    "        \n",
    "        \n",
    "    def parametros(self):\n",
    "        return self.params\n",
    "    \n",
    "    \n",
    "    def param_entorno(self):\n",
    "        return self.params['env']\n",
    "      \n",
    "        \n",
    "    def param_agente(self):\n",
    "        return self.params['agent']\n",
    "    \n",
    "    \n",
    "    def actualiza_param_agente(self, **kwargs):\n",
    "        for k,v in kwargs.items():\n",
    "            if k in self.params.keys():\n",
    "                self.params['agent'][k]=v\n",
    "            \n",
    "            \n",
    "    def exporta_param_entorno(self, archivo):\n",
    "        with open(archivo, 'w') as f:\n",
    "            json.dump(self.params['env'], f, indent=4, separators=(',',': '), sort_keys=True)\n",
    "            f.write('\\n')\n",
    "\n",
    "            \n",
    "    def exporta_param_agente(self, archivo):\n",
    "        with open(archivo, 'w') as f:\n",
    "            json.dump(self.params['agent'], f, indent=4, separators=(',', ': '), sort_keys=True)\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora implementaremos un perceptron de una capa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, dim_entrada, dim_salida, device=torch.device('cpu')):\n",
    "\n",
    "        super(Perceptron, self).__init__()\n",
    "        self.device=device\n",
    "        self.dim_entrada=dim_entrada[0]\n",
    "        self.dim_oculta=40\n",
    "        self.linear=torch.nn.Linear(self.dim_entrada, self.dim_oculta)\n",
    "        self.salida=torch.nn.Linear(self.dim_oculta, dim_salida)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x=torch.from_numpy(x).float().to(self.device)\n",
    "        x=torch.nn.functional.relu(self.linear(x))\n",
    "        x=self.salida(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replay de la experiencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experiencia=namedtuple('Experiencia', ['obs', 'action', 'reward', 'next_obs', 'done'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memoria(object):\n",
    "    '''\n",
    "    Implementacion de un buffer ciclico basado en la memoria de la experiencia\n",
    "    '''\n",
    "    def __init__(self, capacidad=int(1e6)):\n",
    "        \"\"\"\n",
    "        capacidad: Max numero de experiencias\n",
    "        \"\"\"\n",
    "        self.capacidad=capacidad\n",
    "        self.mem_idx=0  # Indice de la experiencia actual\n",
    "        self.memoria=[]\n",
    "\n",
    "        \n",
    "    def guarda(self, experiencia):\n",
    "        '''\n",
    "        experiencia: el objeto a ser guardado en memoria\n",
    "        '''\n",
    "        if self.mem_idx<self.capacidad:\n",
    "            # Extiende la memoria y crea un espacio\n",
    "            self.memoria.append(None)\n",
    "        self.memoria[self.mem_idx%self.capacidad]=experiencia\n",
    "        self.mem_idx+=1\n",
    "\n",
    "        \n",
    "    def muestra(self, batch_size):\n",
    "        '''\n",
    "        batch_size:  tama単o de la muestra\n",
    "        '''\n",
    "        assert batch_size<=len(self.memoria), 'El tama単o de la muestra esta disponible en memoria.'\n",
    "        \n",
    "        # se devuelve una lista de experiencias con muestreo aleatorio\n",
    "        return random.sample(self.memoria, batch_size)\n",
    "\n",
    "    \n",
    "    def tama単o_muestra(self):\n",
    "        return len(self.memoria) # numero de experiencias guardadas en memoria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicializa los pesos con el metodo Xavier. (Xavier Glorot, Yoshua Bengio, \"Understanding the\n",
    "difficulty of training deep feedforward neural networks\").                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xavier(m):\n",
    "    if isinstance(m, torch.nn.Conv2d) or isinstance(m, torch.nn.Linear):\n",
    "        torch.nn.init.xavier_normal_(m.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frames de observacion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Frames(gym.ObservationWrapper):\n",
    "    \n",
    "    def __init__(self, entorno):\n",
    "        \n",
    "        super(ResizeReshapeFrames, self).__init__(entorno)\n",
    "        if len(self.observation_space.shape)==3: \n",
    "            self.ancho=84\n",
    "            self.alto=84\n",
    "            self.canales=self.observation_space.shape[2]\n",
    "            # canales x alto x ancho \n",
    "            self.espacio_obs=gym.spaces.Box(0, 255, (self.canales, \n",
    "                                                     self.alto,\n",
    "                                                     self.ancho), \n",
    "                                            dtype=np.uint8)\n",
    "\n",
    "\n",
    "    def observacion(self, obs):\n",
    "        if len(obs.shape)==3:\n",
    "            obs=cv2.resize(obs, (self.ancho, self.alto))\n",
    "            if obs.shape[2]<obs.shape[0]:\n",
    "                obs=np.reshape(obs, (obs.shape[2], obs.shape[1], obs.shape[0]))\n",
    "        return obs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Entorno (Atari)**\n",
    "\n",
    "Utiles para definir y manejar el entorno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funcion para definir el entorno\n",
    "\n",
    "def entorno(_id, conf):\n",
    "    ent=gym.make(_id)\n",
    "    if 'NoFrameskip' in _id:\n",
    "        assert 'NoFrameskip' in ent.spec.id\n",
    "        ent=NoopResetEnv(ent, noop_max=30)\n",
    "        ent=MaxAndSkipEnv(ent, skip=conf['skip_rate'])\n",
    "\n",
    "    if conf['episodic_life']:\n",
    "        ent=EpisodicLifeEnv(ent)\n",
    "\n",
    "    try:\n",
    "        if 'FIRE' in ent.unwrapped.get_action_meanings():\n",
    "            ent=FireResetEnv(ent)\n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "    ent=AtariReescala(ent, conf['useful_region'])\n",
    "\n",
    "    if conf['normalize_observation']:\n",
    "        ent=NormalizedEnv(ent)\n",
    "\n",
    "    ent=FrameStack(ent, conf['num_frames_to_stack'])\n",
    "\n",
    "    #if conf['clip_reward']:  # La recompensa del clip se hace por el agente usando sus parametros\n",
    "    #    ent=RecompensaClip(ent)\n",
    "    \n",
    "    \n",
    "    return ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lista de juegos\n",
    "\n",
    "def lista_juegos():\n",
    "    return atari_py.list_games()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recompensa dada por clip\n",
    "\n",
    "class RecompensaClip(gym.RewardWrapper):\n",
    "    \n",
    "    def __init__(self, ent):\n",
    "        gym.RewardWrapper.__init__(self, ent)\n",
    "\n",
    "    def recompensa(self, recom):\n",
    "        '''La recompensa del clip puede ser -1, 0 o +1'''  \n",
    "        return np.sign(recom)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# para frames\n",
    "\n",
    "def frame_84(frame, conf):\n",
    "    frame=frame[conf['crop1']:conf['crop2']+160, :160]    # selecciona\n",
    "    frame=frame.mean(2)                                   # media  \n",
    "    #frame = frame.astype(np.float32)                     # a float\n",
    "    #frame *= (1.0 / 255.0)                               # normalizacion\n",
    "    frame=cv2.resize(frame, (84, conf['dimension2']))     # resize\n",
    "    frame=cv2.resize(frame, (84, 84))\n",
    "    frame=np.reshape(frame, [1, 84, 84])                  # reshape\n",
    "    return frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "class AtariReescala(gym.ObservationWrapper):\n",
    "    def __init__(self, env, env_conf):\n",
    "        gym.ObservationWrapper.__init__(self, env)\n",
    "        self.observation_space = Box(0, 255, [1, 84, 84], dtype=np.uint8)\n",
    "        self.conf = env_conf\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return process_frame_84(observation, self.conf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "class NormalizedEnv(gym.ObservationWrapper):\n",
    "    def __init__(self, env=None):\n",
    "        gym.ObservationWrapper.__init__(self, env)\n",
    "        self.state_mean = 0\n",
    "        self.state_std = 0\n",
    "        self.alpha = 0.9999\n",
    "        self.num_steps = 0\n",
    "\n",
    "    def observation(self, observation):\n",
    "        self.num_steps += 1\n",
    "        self.state_mean = self.state_mean * self.alpha + \\\n",
    "            observation.mean() * (1 - self.alpha)\n",
    "        self.state_std = self.state_std * self.alpha + \\\n",
    "            observation.std() * (1 - self.alpha)\n",
    "\n",
    "        unbiased_mean = self.state_mean / (1 - pow(self.alpha, self.num_steps))\n",
    "        unbiased_std = self.state_std / (1 - pow(self.alpha, self.num_steps))\n",
    "\n",
    "        return (observation - unbiased_mean) / (unbiased_std + 1e-8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "class NoopResetEnv(gym.Wrapper):\n",
    "    def __init__(self, env, noop_max=30):\n",
    "        \"\"\"Sample initial states by taking random number of no-ops on reset.\n",
    "        No-op is assumed to be action 0.\n",
    "        \"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.noop_max = noop_max\n",
    "        self.noop_action = 0\n",
    "        assert env.unwrapped.get_action_meanings()[0] == 'NOOP'\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\" Do no-op action for a number of steps in [1, noop_max].\"\"\"\n",
    "        self.env.reset()\n",
    "        noops = random.randrange(1, self.noop_max + 1)  # pylint: disable=E1101\n",
    "        assert noops > 0\n",
    "        obs = None\n",
    "        for _ in range(noops):\n",
    "            obs, _, done, _ = self.env.step(self.noop_action)\n",
    "        return obs\n",
    "\n",
    "    def step(self, ac):\n",
    "        return self.env.step(ac)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "class FireResetEnv(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        \"\"\"Take action on reset for environments that are fixed until firing.\"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
    "        assert len(env.unwrapped.get_action_meanings()) >= 3\n",
    "\n",
    "    def reset(self):\n",
    "        self.env.reset()\n",
    "        obs, _, done, _ = self.env.step(1)\n",
    "        if done:\n",
    "            self.env.reset()\n",
    "        obs, _, done, _ = self.env.step(2)\n",
    "        if done:\n",
    "            self.env.reset()\n",
    "        return obs\n",
    "\n",
    "    def step(self, ac):\n",
    "        return self.env.step(ac)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "class EpisodicLifeEnv(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        \"\"\"Make end-of-life == end-of-episode, but only reset on true game over.\n",
    "        Done by DeepMind for the DQN and co. since it helps value estimation.\n",
    "        \"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.lives = 0\n",
    "        self.was_real_done = True\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "        self.was_real_done = True\n",
    "        # check current lives, make loss of life terminal,\n",
    "        # then update lives to handle bonus lives\n",
    "        lives = info['ale.lives']\n",
    "        if lives < self.lives and lives > 0:\n",
    "            # for Qbert sometimes we stay in lives == 0 condition for a few frames\n",
    "            # so its important to keep lives > 0, so that we only reset once\n",
    "            # the environment advertises done.\n",
    "            done = True\n",
    "            self.was_real_done = False\n",
    "        self.lives = lives\n",
    "        return obs, reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset only when lives are exhausted.\n",
    "        This way all states are still reachable even though lives are episodic,\n",
    "        and the learner need not know about any of this behind-the-scenes.\n",
    "        \"\"\"\n",
    "        if self.was_real_done:\n",
    "            obs = self.env.reset()\n",
    "            self.lives = 0\n",
    "        else:\n",
    "            # no-op step to advance from terminal/lost life state\n",
    "            obs, _, _, info = self.env.step(0)\n",
    "            self.lives = info['ale.lives']\n",
    "        return obs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "class MaxAndSkipEnv(gym.Wrapper):\n",
    "    def __init__(self, env=None, skip=4):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        # most recent raw observations (for max pooling across time steps)\n",
    "        self._obs_buffer = deque(maxlen=2)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        done = None\n",
    "        for _ in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            self._obs_buffer.append(obs)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        max_frame = np.max(np.stack(self._obs_buffer), axis=0)\n",
    "        return max_frame, total_reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Clear past frame buffer and init. to first obs. from inner env.\"\"\"\n",
    "        self._obs_buffer.clear()\n",
    "        obs = self.env.reset()\n",
    "        self._obs_buffer.append(obs)\n",
    "        return obs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "class FrameStack(gym.Wrapper):\n",
    "    def __init__(self, env, k):\n",
    "        \"\"\"Stack k last frames.\n",
    "\n",
    "        Returns lazy array, which is much more memory efficient.\n",
    "        From baselines atari_wrapper\n",
    "        \"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.k = k\n",
    "        self.frames = deque([], maxlen=k)\n",
    "        shp = env.observation_space.shape\n",
    "        self.observation_space = Box(low=0, high=255, shape=(shp[0] * k , shp[1], shp[2]), dtype=np.uint8)\n",
    "\n",
    "    def reset(self):\n",
    "        ob = self.env.reset()\n",
    "        for _ in range(self.k):\n",
    "            self.frames.append(ob)\n",
    "        return self._get_ob()\n",
    "\n",
    "    def step(self, action):\n",
    "        ob, reward, done, info = self.env.step(action)\n",
    "        self.frames.append(ob)\n",
    "        return self._get_ob(), reward, done, info\n",
    "\n",
    "    def _get_ob(self):\n",
    "        assert len(self.frames) == self.k\n",
    "        return LazyFrames(list(self.frames))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "class LazyFrames(object):\n",
    "    def __init__(self, frames):\n",
    "        \"\"\"This object ensures that common frames between the observations are only stored once.\n",
    "        It exists purely to optimize memory usage which can be huge for DQN's 1M frames replay\n",
    "        buffers.\n",
    "        This object should only be converted to numpy array before being passed to the model.\n",
    "        \"\"\"\n",
    "        self._frames = frames\n",
    "        self._out = None\n",
    "\n",
    "    def _force(self):\n",
    "        if self._out is None:\n",
    "            self._out = np.concatenate(self._frames, axis=0)\n",
    "            self._frames = None\n",
    "        return self._out\n",
    "\n",
    "    def __array__(self, dtype=None):\n",
    "        out = self._force()\n",
    "        if dtype is not None:\n",
    "            out = out.astype(dtype)\n",
    "        return out\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._force())\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self._force()[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
