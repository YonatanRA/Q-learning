{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cart Pole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Descripcion \n",
    "\n",
    "\n",
    "+ **Espacio de observacion (Box(4,))**\n",
    "\n",
    "+ Posicion del carro: [-2.4, 2.4]\n",
    "+ Velocidad del carro: [$-\\infty$, $\\infty$]\n",
    "+ Angulo del palo: [-41.8, 41.8]\n",
    "+ Velocidad del palo en la punta: [$-\\infty$, $\\infty$]\n",
    "\n",
    "\n",
    "+ **Espacio de accion (Discrete(2))**\n",
    "\n",
    "+ Izquierda: 0\n",
    "+ Derecha: 1\n",
    "\n",
    "\n",
    "El objetivo es mantener el palo vertical moviendo a izquierda y derecha el carro.\n",
    "\n",
    "\n",
    "La recompensa es +1 para cada paso temporal. El episodio se termina si el angulo es mayor a $\\pm 12$ grados o si el carro sobrepasa la posicion $\\pm 2.4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# primero test aleatorio del entorno\n",
    "\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "entorno=gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episodio #1 finalizado en 37 pasos. Recompensa total:37.0\n",
      "\n",
      "Episodio #2 finalizado en 20 pasos. Recompensa total:20.0\n",
      "\n",
      "Episodio #3 finalizado en 29 pasos. Recompensa total:29.0\n",
      "\n",
      "Episodio #4 finalizado en 35 pasos. Recompensa total:35.0\n",
      "\n",
      "Episodio #5 finalizado en 16 pasos. Recompensa total:16.0\n"
     ]
    }
   ],
   "source": [
    "for episodio in range(5):  # 5 episodios\n",
    "    \n",
    "    done=False\n",
    "    \n",
    "    observacion=entorno.reset()  # observacion\n",
    "    \n",
    "    recompensa_total=0.          # recompensa total en cada episodio\n",
    "    \n",
    "    paso=0                       # paso en cada episodio\n",
    "     \n",
    "    while not done:\n",
    "        entorno.render()  # muestra el entorno\n",
    "        \n",
    "        accion=entorno.action_space.sample() # accion aleatoria, se cambiara por el agente\n",
    "        \n",
    "        siguiente_estado, recompensa, done, info=entorno.step(accion)\n",
    "        \n",
    "        recompensa_total+=recompensa\n",
    "        \n",
    "        paso+=1\n",
    "        \n",
    "        observacion=siguiente_estado\n",
    "        \n",
    "    print ('\\nEpisodio #{} finalizado en {} pasos. Recompensa total:{}'.format(episodio+1, paso, recompensa_total))\n",
    "    \n",
    "entorno.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se usara un perceptron desde SkLearn, de una manera mas simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_actions = env.action_space.n\n",
    "\n",
    "\n",
    "#Create agent\n",
    "agent = MLPClassifier(hidden_layer_sizes=(20,20),\n",
    "                      activation='tanh',\n",
    "                      solver='adam',\n",
    "                      warm_start=True,max_iter=1\n",
    "                     )\n",
    "#initialize agent by feeding it with some random bullshit\n",
    "agent.fit([env.reset()]*n_actions,range(n_actions));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_session():\n",
    "    \"\"\"\n",
    "    Just ask agent to predict action and see how env reacts - repeat until exhaustion.\n",
    "    :param greedy: if True, picks most likely actions, else samples actions\"\"\"\n",
    "    states,actions,total_reward = [],[],0\n",
    "    \n",
    "    s = env.reset()    \n",
    "    while True:\n",
    "        a = np.random.choice(n_actions,p=agent.predict_proba([s])[0])\n",
    "        \n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        \n",
    "        s,r,done,_ = env.step(a)\n",
    "        total_reward+=r\n",
    "        if done:break\n",
    "        \n",
    "    return states,actions,total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#training loop\n",
    "n_samples = 100 #take 100 samples\n",
    "percentile = 70 #fit on top 30% (30 best samples)\n",
    "\n",
    "for i in range(50):\n",
    "    #sample sessions\n",
    "    sessions = [generate_session() for _ in range(n_samples)]\n",
    "    batch_states,batch_actions,batch_rewards = map(np.array,zip(*sessions))\n",
    "    \n",
    "    #choose threshold on rewards\n",
    "    threshold = np.percentile(batch_rewards,percentile)\n",
    "    elite_states = np.concatenate(batch_states[batch_rewards>=threshold])\n",
    "    elite_actions = np.concatenate(batch_actions[batch_rewards>=threshold])\n",
    "    \n",
    "    #fit our osom neural network >.<\n",
    "    agent.fit(elite_states,elite_actions)\n",
    "\n",
    "    #report progress\n",
    "    print(\"epoch %i \\tmean reward=%.2f\\tthreshold=%.2f\"%(i,batch_rewards.mean(),threshold))\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
