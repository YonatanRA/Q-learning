{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cart Pole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Descripcion \n",
    "\n",
    "\n",
    "+ **Espacio de observacion (Box(4,))**\n",
    "\n",
    "+ Posicion del carro: [-2.4, 2.4]\n",
    "+ Velocidad del carro: [$-\\infty$, $\\infty$]\n",
    "+ Angulo del palo: [-41.8, 41.8]\n",
    "+ Velocidad del palo en la punta: [$-\\infty$, $\\infty$]\n",
    "\n",
    "\n",
    "+ **Espacio de accion (Discrete(2))**\n",
    "\n",
    "+ Izquierda: 0\n",
    "+ Derecha: 1\n",
    "\n",
    "\n",
    "El objetivo es mantener el palo vertical moviendo a izquierda y derecha el carro.\n",
    "\n",
    "\n",
    "La recompensa es +1 para cada paso temporal. El episodio se termina si el angulo es mayor a $\\pm 12$ grados o si el carro sobrepasa la posicion $\\pm 2.4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# primero test aleatorio del entorno\n",
    "\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "entorno=gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episodio #1 finalizado en 29 pasos. Recompensa total:29.0\n",
      "\n",
      "Episodio #2 finalizado en 21 pasos. Recompensa total:21.0\n",
      "\n",
      "Episodio #3 finalizado en 13 pasos. Recompensa total:13.0\n",
      "\n",
      "Episodio #4 finalizado en 20 pasos. Recompensa total:20.0\n",
      "\n",
      "Episodio #5 finalizado en 18 pasos. Recompensa total:18.0\n"
     ]
    }
   ],
   "source": [
    "for episodio in range(5):  # 5 episodios\n",
    "    \n",
    "    done=False\n",
    "    \n",
    "    observacion=entorno.reset()  # observacion\n",
    "    \n",
    "    recompensa_total=0.          # recompensa total en cada episodio\n",
    "    \n",
    "    paso=0                       # paso en cada episodio\n",
    "     \n",
    "    while not done:\n",
    "        entorno.render()  # muestra el entorno\n",
    "        \n",
    "        accion=entorno.action_space.sample() # accion aleatoria, se cambiara por el agente\n",
    "        \n",
    "        siguiente_estado, recompensa, done, info=entorno.step(accion)\n",
    "        \n",
    "        recompensa_total+=recompensa\n",
    "        \n",
    "        paso+=1\n",
    "        \n",
    "        observacion=siguiente_estado\n",
    "        \n",
    "    print ('\\nEpisodio #{} finalizado en {} pasos. Recompensa total:{}'.format(episodio+1, paso, recompensa_total))\n",
    "    \n",
    "entorno.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Se usara un perceptron desde SkLearn, de una manera mas simple**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_accion=entorno.action_space.n\n",
    "\n",
    "\n",
    "#Se crea el agente\n",
    "agente=MLPClassifier(hidden_layer_sizes=(40,40), activation='relu',\n",
    "                     solver='adam', warm_start=True, max_iter=50)\n",
    "\n",
    "\n",
    "#Se entrena el agente de manera aleatoria\n",
    "agente.fit([entorno.reset()]*dim_accion,range(dim_accion));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sesion():\n",
    "\n",
    "    observaciones=[]\n",
    "    acciones=[]\n",
    "    recompensa_total=0.\n",
    "    \n",
    "    observacion=entorno.reset()   \n",
    "    \n",
    "    while 1:\n",
    "        accion=np.random.choice(dim_accion, p=agente.predict_proba([observacion])[0])\n",
    "        \n",
    "        observaciones.append(observacion)\n",
    "        acciones.append(accion)\n",
    "        \n",
    "        siguiente_obs, recompensa, done, info=entorno.step(accion)\n",
    "        recompensa_total+=recompensa\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "    return observaciones, acciones, recompensa_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio: 0 \tRecompensa Media=19.87\tRecompensa Max=45.0\tUmbral=23.0\n",
      "Episodio: 1 \tRecompensa Media=21.22\tRecompensa Max=56.0\tUmbral=24.299999999999997\n",
      "Episodio: 2 \tRecompensa Media=22.21\tRecompensa Max=110.0\tUmbral=22.0\n",
      "Episodio: 3 \tRecompensa Media=23.56\tRecompensa Max=73.0\tUmbral=24.0\n",
      "Episodio: 4 \tRecompensa Media=21.22\tRecompensa Max=92.0\tUmbral=22.299999999999997\n",
      "Episodio: 5 \tRecompensa Media=22.87\tRecompensa Max=74.0\tUmbral=25.299999999999997\n",
      "Episodio: 6 \tRecompensa Media=22.59\tRecompensa Max=82.0\tUmbral=28.0\n",
      "Episodio: 7 \tRecompensa Media=24.16\tRecompensa Max=113.0\tUmbral=27.299999999999997\n",
      "Episodio: 8 \tRecompensa Media=23.9\tRecompensa Max=80.0\tUmbral=27.299999999999997\n",
      "Episodio: 9 \tRecompensa Media=20.5\tRecompensa Max=56.0\tUmbral=22.299999999999997\n",
      "Episodio: 10 \tRecompensa Media=25.63\tRecompensa Max=86.0\tUmbral=30.0\n",
      "Episodio: 11 \tRecompensa Media=20.97\tRecompensa Max=88.0\tUmbral=25.0\n",
      "Episodio: 12 \tRecompensa Media=22.07\tRecompensa Max=57.0\tUmbral=24.299999999999997\n",
      "Episodio: 13 \tRecompensa Media=22.3\tRecompensa Max=74.0\tUmbral=26.0\n",
      "Episodio: 14 \tRecompensa Media=21.04\tRecompensa Max=86.0\tUmbral=23.0\n",
      "Episodio: 15 \tRecompensa Media=22.83\tRecompensa Max=78.0\tUmbral=24.0\n",
      "Episodio: 16 \tRecompensa Media=23.08\tRecompensa Max=77.0\tUmbral=24.0\n",
      "Episodio: 17 \tRecompensa Media=20.8\tRecompensa Max=72.0\tUmbral=25.0\n",
      "Episodio: 18 \tRecompensa Media=20.45\tRecompensa Max=71.0\tUmbral=22.299999999999997\n",
      "Episodio: 19 \tRecompensa Media=25.01\tRecompensa Max=84.0\tUmbral=27.0\n",
      "Episodio: 20 \tRecompensa Media=22.64\tRecompensa Max=60.0\tUmbral=25.0\n",
      "Episodio: 21 \tRecompensa Media=22.88\tRecompensa Max=77.0\tUmbral=26.299999999999997\n",
      "Episodio: 22 \tRecompensa Media=21.25\tRecompensa Max=69.0\tUmbral=23.0\n",
      "Episodio: 23 \tRecompensa Media=24.06\tRecompensa Max=69.0\tUmbral=26.0\n",
      "Episodio: 24 \tRecompensa Media=22.77\tRecompensa Max=69.0\tUmbral=25.299999999999997\n",
      "Episodio: 25 \tRecompensa Media=22.37\tRecompensa Max=76.0\tUmbral=25.0\n",
      "Episodio: 26 \tRecompensa Media=22.85\tRecompensa Max=66.0\tUmbral=25.299999999999997\n",
      "Episodio: 27 \tRecompensa Media=24.06\tRecompensa Max=81.0\tUmbral=29.299999999999997\n",
      "Episodio: 28 \tRecompensa Media=22.21\tRecompensa Max=75.0\tUmbral=25.0\n",
      "Episodio: 29 \tRecompensa Media=22.59\tRecompensa Max=76.0\tUmbral=24.0\n",
      "Episodio: 30 \tRecompensa Media=21.6\tRecompensa Max=78.0\tUmbral=22.0\n",
      "Episodio: 31 \tRecompensa Media=24.48\tRecompensa Max=63.0\tUmbral=28.0\n",
      "Episodio: 32 \tRecompensa Media=22.22\tRecompensa Max=75.0\tUmbral=25.299999999999997\n",
      "Episodio: 33 \tRecompensa Media=22.26\tRecompensa Max=67.0\tUmbral=24.0\n",
      "Episodio: 34 \tRecompensa Media=22.95\tRecompensa Max=80.0\tUmbral=26.0\n",
      "Episodio: 35 \tRecompensa Media=22.44\tRecompensa Max=78.0\tUmbral=25.299999999999997\n",
      "Episodio: 36 \tRecompensa Media=21.16\tRecompensa Max=80.0\tUmbral=25.0\n",
      "Episodio: 37 \tRecompensa Media=21.82\tRecompensa Max=69.0\tUmbral=23.299999999999997\n",
      "Episodio: 38 \tRecompensa Media=23.16\tRecompensa Max=78.0\tUmbral=24.299999999999997\n",
      "Episodio: 39 \tRecompensa Media=22.94\tRecompensa Max=96.0\tUmbral=25.0\n",
      "Episodio: 40 \tRecompensa Media=21.78\tRecompensa Max=90.0\tUmbral=24.0\n",
      "Episodio: 41 \tRecompensa Media=23.18\tRecompensa Max=71.0\tUmbral=27.299999999999997\n",
      "Episodio: 42 \tRecompensa Media=22.95\tRecompensa Max=91.0\tUmbral=25.0\n",
      "Episodio: 43 \tRecompensa Media=22.0\tRecompensa Max=57.0\tUmbral=25.0\n",
      "Episodio: 44 \tRecompensa Media=24.28\tRecompensa Max=92.0\tUmbral=26.0\n",
      "Episodio: 45 \tRecompensa Media=21.33\tRecompensa Max=56.0\tUmbral=26.0\n",
      "Episodio: 46 \tRecompensa Media=21.37\tRecompensa Max=73.0\tUmbral=23.299999999999997\n",
      "Episodio: 47 \tRecompensa Media=19.86\tRecompensa Max=45.0\tUmbral=24.0\n",
      "Episodio: 48 \tRecompensa Media=20.31\tRecompensa Max=83.0\tUmbral=21.0\n",
      "Episodio: 49 \tRecompensa Media=21.74\tRecompensa Max=57.0\tUmbral=25.0\n"
     ]
    }
   ],
   "source": [
    "# Loop de entrenamiento\n",
    "\n",
    "pasos=100       # se toman 100 pasos\n",
    "percentil=70    # se entrena en el mejor 30% (30 mejores pasos)\n",
    "\n",
    "\n",
    "for i in range(50):  # 50 episodios\n",
    "\n",
    "    sesiones=[sesion() for _ in range(pasos)]\n",
    "    \n",
    "    batch_observaciones, batch_acciones, batch_recompensas=map(np.array,zip(*sesiones))\n",
    "    \n",
    "    # Se escoge el umbral de recompensas\n",
    "    umbral=np.percentile(batch_recompensas, percentil)\n",
    "    \n",
    "    mejores_observaciones=np.concatenate(batch_observaciones[batch_recompensas>=umbral])\n",
    "    mejores_acciones=np.concatenate(batch_acciones[batch_recompensas>=umbral])\n",
    "    \n",
    "    # Se entrena con lo mejor\n",
    "    agente.fit(mejores_observaciones, mejores_acciones)\n",
    "\n",
    "    print('Episodio: {} \\tRecompensa Media={}\\tRecompensa Max={}\\tUmbral={}'.format(i, batch_recompensas.mean(), batch_recompensas.max(), umbral))\n",
    "    \n",
    "entorno.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
