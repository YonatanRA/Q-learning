{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Herramientas para creacion de agente DQL con red convolucional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a crear algunas herramientas para un agente con una red neuronal convolucional que sea capaz de reconocer imagenes como datos de entrada. Entrenaremos al agente con juegos de Atari, por lo que tambien crearemos las herramientas del entorno.\n",
    "\n",
    "Recordemos primero el decay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import atari_py\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from gym.spaces.box import Box\n",
    "import cv2\n",
    "import random\n",
    "import torch\n",
    "import json\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecayLineal(object):\n",
    "    def __init__(self, valor_ini, valor_final, pasos_max):\n",
    "        assert valor_ini>valor_final, 'valor inicial > valor final'\n",
    "        self.valor_ini=valor_ini\n",
    "        self.valor_final=valor_final\n",
    "        self.decay=(valor_ini-valor_final)/pasos_max\n",
    "\n",
    "    def __call__(self, num_pasos):\n",
    "        valor_actual=self.valor_ini-self.decay*num_pasos\n",
    "        if valor_actual<self.valor_final:\n",
    "            valor_actual=self.valor_final\n",
    "        return valor_actual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seguiremos implementando la red convolucional con Torch de 3 capas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, dim_entrada, dim_salida, device='cpu'): # device=cpu o cuda\n",
    "        \n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        self.device=device\n",
    "        \n",
    "        self.capa1=torch.nn.Sequential(\n",
    "                    torch.nn.Conv2d(dim_entrad[0],\n",
    "                                    64,\n",
    "                                    kernel_size=4,\n",
    "                                    stride=2,\n",
    "                                    padding=1),\n",
    "                    torch.nn.ReLU())\n",
    "        \n",
    "        \n",
    "        self.capa2=torch.nn.Sequential(\n",
    "                    torch.nn.Conv2d(64,\n",
    "                                    32,\n",
    "                                    kernel_size=4,\n",
    "                                    stride=2,\n",
    "                                    padding=0),\n",
    "                    torch.nn.ReLU())\n",
    "        \n",
    "        \n",
    "        self.capa3=torch.nn.Sequential(\n",
    "                    torch.nn.Conv2d(32,\n",
    "                                    32,\n",
    "                                    kernel_size=3,\n",
    "                                    stride=2,\n",
    "                                    padding=0),\n",
    "                    torch.nn.ReLU())\n",
    "        \n",
    "        self.salida=torch.nn.Linear(18*18*32, dim_salida)\n",
    "        \n",
    "       \n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x=torch.from_numpy(x).float().to(self.device)\n",
    "        x=self.capa1(x)\n",
    "        x=self.capa2(x)\n",
    "        x=self.capa3(x)\n",
    "        x=x.view(x.shape[0], -1)\n",
    "        x=self.salida(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podriamos a単adir mas capas a la red.\n",
    "\n",
    "Ahora que hemos creado la red convolucional, vamos a crear un controlador de hiperparametros a traves de un JSON, siendo de esta manera mas facil manejar la red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParamControl(object):\n",
    "    \n",
    "    def __init__(self, archivo): # archivo json, path\n",
    "        \n",
    "        self.params=json.load(open(archivo, 'r'))\n",
    "        \n",
    "        \n",
    "    def parametros(self):\n",
    "        return self.params\n",
    "    \n",
    "    \n",
    "    def param_entorno(self):\n",
    "        return self.params['env']\n",
    "      \n",
    "        \n",
    "    def param_agente(self):\n",
    "        return self.params['agent']\n",
    "    \n",
    "    \n",
    "    def actualiza_param_agente(self, **kwargs):\n",
    "        for k,v in kwargs.items():\n",
    "            if k in self.params.keys():\n",
    "                self.params['agent'][k]=v\n",
    "            \n",
    "            \n",
    "    def exporta_param_entorno(self, archivo):\n",
    "        with open(archivo, 'w') as f:\n",
    "            json.dump(self.params['env'], f, indent=4, separators=(',',': '), sort_keys=True)\n",
    "            f.write('\\n')\n",
    "\n",
    "            \n",
    "    def exporta_param_agente(self, archivo):\n",
    "        with open(archivo, 'w') as f:\n",
    "            json.dump(self.params['agent'], f, indent=4, separators=(',', ': '), sort_keys=True)\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora implementaremos un perceptron de una capa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, dim_entrada, dim_salida, device=torch.device('cpu')):\n",
    "\n",
    "        super(Perceptron, self).__init__()\n",
    "        self.device=device\n",
    "        self.dim_entrada=dim_entrada[0]\n",
    "        self.dim_oculta=40\n",
    "        self.linear=torch.nn.Linear(self.dim_entrada, self.dim_oculta)\n",
    "        self.salida=torch.nn.Linear(self.dim_oculta, dim_salida)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x=torch.from_numpy(x).float().to(self.device)\n",
    "        x=torch.nn.functional.relu(self.linear(x))\n",
    "        x=self.salida(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replay de la experiencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experiencia=namedtuple('Experiencia', ['obs', 'action', 'reward', 'next_obs', 'done'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memoria(object):\n",
    "    '''\n",
    "    Implementacion de un buffer ciclico basado en la memoria de la experiencia\n",
    "    '''\n",
    "    def __init__(self, capacidad=int(1e6)):\n",
    "        \"\"\"\n",
    "        capacidad: Max numero de experiencias\n",
    "        \"\"\"\n",
    "        self.capacidad=capacidad\n",
    "        self.mem_idx=0  # Indice de la experiencia actual\n",
    "        self.memoria=[]\n",
    "\n",
    "        \n",
    "    def guarda(self, experiencia):\n",
    "        '''\n",
    "        experiencia: el objeto a ser guardado en memoria\n",
    "        '''\n",
    "        if self.mem_idx<self.capacidad:\n",
    "            # Extiende la memoria y crea un espacio\n",
    "            self.memoria.append(None)\n",
    "        self.memoria[self.mem_idx%self.capacidad]=experiencia\n",
    "        self.mem_idx+=1\n",
    "\n",
    "        \n",
    "    def muestra(self, batch_size):\n",
    "        '''\n",
    "        batch_size:  tama単o de la muestra\n",
    "        '''\n",
    "        assert batch_size<=len(self.memoria), 'El tama単o de la muestra esta disponible en memoria.'\n",
    "        \n",
    "        # se devuelve una lista de experiencias con muestreo aleatorio\n",
    "        return random.sample(self.memoria, batch_size)\n",
    "\n",
    "    \n",
    "    def tama単o_muestra(self):\n",
    "        return len(self.memoria) # numero de experiencias guardadas en memoria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicializa los pesos con el metodo Xavier. (Xavier Glorot, Yoshua Bengio, \"Understanding the\n",
    "difficulty of training deep feedforward neural networks\").                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xavier(m):\n",
    "    if isinstance(m, torch.nn.Conv2d) or isinstance(m, torch.nn.Linear):\n",
    "        torch.nn.init.xavier_normal_(m.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frames de observacion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Frames(gym.ObservationWrapper):\n",
    "    \n",
    "    def __init__(self, entorno):\n",
    "        \n",
    "        super(ResizeReshapeFrames, self).__init__(entorno)\n",
    "        if len(self.observation_space.shape)==3: \n",
    "            self.ancho=84\n",
    "            self.alto=84\n",
    "            self.canales=self.observation_space.shape[2]\n",
    "            # canales x alto x ancho \n",
    "            self.espacio_obs=gym.spaces.Box(0, 255, (self.canales, \n",
    "                                                     self.alto,\n",
    "                                                     self.ancho), \n",
    "                                            dtype=np.uint8)\n",
    "\n",
    "\n",
    "    def observacion(self, obs):\n",
    "        if len(obs.shape)==3:\n",
    "            obs=cv2.resize(obs, (self.ancho, self.alto))\n",
    "            if obs.shape[2]<obs.shape[0]:\n",
    "                obs=np.reshape(obs, (obs.shape[2], obs.shape[1], obs.shape[0]))\n",
    "        return obs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Entorno (Atari)**\n",
    "\n",
    "Utiles para definir y manejar el entorno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funcion para definir el entorno\n",
    "\n",
    "def entorno(_id, conf):\n",
    "    ent=gym.make(_id)\n",
    "    if 'NoFrameskip' in _id:\n",
    "        assert 'NoFrameskip' in ent.spec.id\n",
    "        ent=NResetEnt(ent, noop_max=30)\n",
    "        ent=MaxSkipEnt(ent, skip=conf['skip_rate'])\n",
    "\n",
    "    if conf['episodic_life']:\n",
    "        ent=VidaEpisodioEnt(ent)\n",
    "\n",
    "    try:\n",
    "        if 'FIRE' in ent.unwrapped.get_action_meanings():\n",
    "            ent=FResetEnv(ent)\n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "    ent=AtariReescala(ent, conf['useful_region'])\n",
    "\n",
    "    if conf['normalize_observation']:\n",
    "        ent=NormalizaEnt(ent)\n",
    "\n",
    "    ent=FrameStack(ent, conf['num_frames_to_stack'])\n",
    "\n",
    "    #if conf['clip_reward']:  # La recompensa del clip se hace por el agente usando sus parametros\n",
    "    #    ent=RecompensaClip(ent)\n",
    "    \n",
    "    \n",
    "    return ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lista de juegos\n",
    "\n",
    "def lista_juegos():\n",
    "    return atari_py.list_games()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recompensa dada por clip\n",
    "\n",
    "class RecompensaClip(gym.RewardWrapper):\n",
    "    \n",
    "    def __init__(self, ent):\n",
    "        gym.RewardWrapper.__init__(self, ent)\n",
    "\n",
    "        \n",
    "    def recompensa(self, recom):\n",
    "        '''La recompensa del clip puede ser -1, 0 o +1'''  \n",
    "        return np.sign(recom)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# para frames\n",
    "\n",
    "def frame_84(frame, conf):\n",
    "    frame=frame[conf['crop1']:conf['crop2']+160, :160]    # selecciona\n",
    "    frame=frame.mean(2)                                   # media  \n",
    "    #frame=frame.astype(np.float32)                       # a float\n",
    "    #frame*=(1./255.)                                     # normalizacion\n",
    "    frame=cv2.resize(frame, (84, conf['dimension2']))     # resize\n",
    "    frame=cv2.resize(frame, (84, 84))\n",
    "    frame=np.reshape(frame, [1, 84, 84])                  # reshape\n",
    "    return frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reescalado de los frames\n",
    "\n",
    "class AtariReescala(gym.ObservationWrapper):\n",
    "    \n",
    "    def __init__(self, ent, conf):\n",
    "        gym.ObservationWrapper.__init__(self, ent)\n",
    "        \n",
    "        self.observation_space=Box(0, 255, [1, 84, 84], dtype=np.uint8)\n",
    "        self.conf=onf\n",
    "\n",
    "    def observacion(self, observacion):\n",
    "        return frame_84(observacion, self.conf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizacion entorno\n",
    "\n",
    "class NormalizaEnt(gym.ObservationWrapper):\n",
    "    \n",
    "    def __init__(self, ent=None):\n",
    "        gym.ObservationWrapper.__init__(self, ent)\n",
    "        \n",
    "        self.media=0\n",
    "        self.std=0\n",
    "        self.alfa=0.9999\n",
    "        self.num_pasos=0\n",
    "\n",
    "    def observacion(self, observacion):\n",
    "        self.num_pasos+=1\n",
    "        self.media=self.media*self.alfa+observacion.mean()*(1-self.alfa)\n",
    "        self.std=self.std*self.alfa+observacion.std()*(1-self.alfa)\n",
    "\n",
    "        sin_bias_media=self.media/(1-pow(self.alfa, self.num_pasos))\n",
    "        sin_bias_std=self.std/(1-pow(self.alfa, self.num_pasos))\n",
    "\n",
    "        return (observacion-sin_bias_media)/(sin_bias_std+1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noop reset del entorno\n",
    "\n",
    "class NResetEnt(gym.Wrapper):\n",
    "    \n",
    "    def __init__(self, ent, noop_max=30):\n",
    "        ' No-op se asume que es accion 0.'\n",
    "        gym.Wrapper.__init__(self, ent)\n",
    "        self.n_max=noop_max\n",
    "        self.n_accion=0\n",
    "        assert ent.unwrapped.get_action_meanings()[0]=='NOOP'\n",
    "\n",
    "    def reset(self):\n",
    "        ' Accion no-op para un numero de pasos en [1, noop_max].'\n",
    "        self.ent.reset()\n",
    "        noops=random.randrange(1, self.n_max+1)  \n",
    "        assert noops>0\n",
    "        obs=None\n",
    "        \n",
    "        for _ in range(noops):\n",
    "            obs, _, done, _ = self.ent.step(self.n_action)\n",
    "        return obs\n",
    "\n",
    "    def paso(self, ac):\n",
    "        return self.ent.step(ac)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fire reset del entorno\n",
    "\n",
    "class FResetEnt(gym.Wrapper):\n",
    "    def __init__(self, ent):\n",
    "        'Toma la accion en el reset para entornos que estan fijos hasta el fire.'\n",
    "        gym.Wrapper.__init__(self, ent)\n",
    "        assert ent.unwrapped.get_action_meanings()[1]=='FIRE'\n",
    "        assert len(ent.unwrapped.get_action_meanings())>=3\n",
    "\n",
    "    def reset(self):\n",
    "        self.ent.reset()\n",
    "        obs, _, done, _=self.ent.step(1)\n",
    "        if done:\n",
    "            self.ent.reset()\n",
    "        obs, _, done, _=self.ent.step(2)\n",
    "        if done:\n",
    "            self.ent.reset()\n",
    "        return obs\n",
    "\n",
    "    def paso(self, ac):\n",
    "        return self.ent.step(ac)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "class VidaEpisodioEnt(gym.Wrapper):\n",
    "    def __init__(self, ent):\n",
    "        'Se hace fin-de-vida==fin-de-episodio, pero solo se resetea en el verdadero fin de juego.'\n",
    "        \n",
    "        gym.Wrapper.__init__(self, ent)\n",
    "        self.vidas=0\n",
    "        self.verdadero_fin=True\n",
    "\n",
    "    def paso(self, accion):\n",
    "        obs, recompensa, done, info=self.ent.step(accion)\n",
    "        self.verdadero_fin=True\n",
    "        # check current lives, make loss of life terminal,\n",
    "        # then update lives to handle bonus lives\n",
    "        vidas=info['ale.lives']\n",
    "        if vidas<self.vidas and vidas>0:\n",
    "            # para el Qbert a veces es vidas==0 la condicion para unos pocos frames\n",
    "            # asi que es importante mantener las vidas>0, se resetea cuendo el entorno devuelve done.\n",
    "            done=True\n",
    "            self.verdadero_fin=False\n",
    "        self.vidas=vidas\n",
    "        return obs, recompensa, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset only when lives are exhausted.\n",
    "        This way all states are still reachable even though lives are episodic,\n",
    "        and the learner need not know about any of this behind-the-scenes.\n",
    "        \"\"\"\n",
    "        if self.verdadero_fin:\n",
    "            obs=self.ent.reset()\n",
    "            self.vidas=0\n",
    "        else:\n",
    "            # no-op step to advance from terminal/lost life state\n",
    "            obs, _, _, info=self.ent.step(0)\n",
    "            self.vidas=info['ale.lives']\n",
    "        return obs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "class MaxSkipEnt(gym.Wrapper):\n",
    "    def __init__(self, env=None, skip=4):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        # most recent raw observations (for max pooling across time steps)\n",
    "        self._obs_buffer = deque(maxlen=2)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        done = None\n",
    "        for _ in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            self._obs_buffer.append(obs)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        max_frame = np.max(np.stack(self._obs_buffer), axis=0)\n",
    "        return max_frame, total_reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Clear past frame buffer and init. to first obs. from inner env.\"\"\"\n",
    "        self._obs_buffer.clear()\n",
    "        obs = self.env.reset()\n",
    "        self._obs_buffer.append(obs)\n",
    "        return obs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "class FrameStack(gym.Wrapper):\n",
    "    def __init__(self, env, k):\n",
    "        \"\"\"Stack k last frames.\n",
    "\n",
    "        Returns lazy array, which is much more memory efficient.\n",
    "        From baselines atari_wrapper\n",
    "        \"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.k = k\n",
    "        self.frames = deque([], maxlen=k)\n",
    "        shp = env.observation_space.shape\n",
    "        self.observation_space = Box(low=0, high=255, shape=(shp[0] * k , shp[1], shp[2]), dtype=np.uint8)\n",
    "\n",
    "    def reset(self):\n",
    "        ob = self.env.reset()\n",
    "        for _ in range(self.k):\n",
    "            self.frames.append(ob)\n",
    "        return self._get_ob()\n",
    "\n",
    "    def step(self, action):\n",
    "        ob, reward, done, info = self.env.step(action)\n",
    "        self.frames.append(ob)\n",
    "        return self._get_ob(), reward, done, info\n",
    "\n",
    "    def _get_ob(self):\n",
    "        assert len(self.frames) == self.k\n",
    "        return LazyFrames(list(self.frames))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "class LazyFrames(object):\n",
    "    def __init__(self, frames):\n",
    "        \"\"\"This object ensures that common frames between the observations are only stored once.\n",
    "        It exists purely to optimize memory usage which can be huge for DQN's 1M frames replay\n",
    "        buffers.\n",
    "        This object should only be converted to numpy array before being passed to the model.\n",
    "        \"\"\"\n",
    "        self._frames = frames\n",
    "        self._out = None\n",
    "\n",
    "    def _force(self):\n",
    "        if self._out is None:\n",
    "            self._out = np.concatenate(self._frames, axis=0)\n",
    "            self._frames = None\n",
    "        return self._out\n",
    "\n",
    "    def __array__(self, dtype=None):\n",
    "        out = self._force()\n",
    "        if dtype is not None:\n",
    "            out = out.astype(dtype)\n",
    "        return out\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._force())\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self._force()[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
