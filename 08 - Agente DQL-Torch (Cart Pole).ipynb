{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cart Pole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Descripcion \n",
    "\n",
    "\n",
    "+ **Espacio de observacion (Box(4,))**\n",
    "\n",
    "+ Posicion del carro: [-2.4, 2.4]\n",
    "+ Velocidad del carro: [$-\\infty$, $\\infty$]\n",
    "+ Angulo del palo: [-41.8, 41.8]\n",
    "+ Velocidad del palo en la punta: [$-\\infty$, $\\infty$]\n",
    "\n",
    "\n",
    "+ **Espacio de accion (Discrete(2))**\n",
    "\n",
    "+ Izquierda: 0\n",
    "+ Derecha: 1\n",
    "\n",
    "\n",
    "El objetivo es mantener el palo vertical moviendo a izquierda y derecha el carro.\n",
    "\n",
    "\n",
    "La recompensa es +1 para cada paso temporal. El episodio se termina si el angulo es mayor a $\\pm 12$ grados o si el carro sobrepasa la posicion $\\pm 2.4$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementaremos un perceptron en torch (red neuronal) para calcular los Q-valores, en vez de usar el algoritmo Q-learning usado anteriormente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perceptron implementado en Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Perceptron(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, dim_entrada, dim_salida, hardware=torch.device('cpu')):\n",
    "        super(Perceptron, self).__init__()                               # herencia\n",
    "        self.hardware=hardware                                           # hardware usado por el perceptron\n",
    "        self.dim_entrada=dim_entrada[0]                                  # dimension de la capa de entrada\n",
    "        self.dim_oculta=50                                               # dimension de la capa oculta del perceptron (50 nodos, se puede cambiar)\n",
    "        self.lineal=torch.nn.Linear(self.dim_entrada, self.dim_oculta)   # entrada (transformaacion lineal)\n",
    "        self.salida=torch.nn.Linear(self.dim_oculta, dim_salida)         # salida\n",
    "        \n",
    "    def forward(self, x):  # metodo de creacion del perceptron\n",
    "        x=torch.from_numpy(np.array(x)).float().to(self.hardware)\n",
    "        x=torch.nn.functional.relu(self.lineal(x)) # unidad rectificado lineal\n",
    "        x=self.salida(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clase Agente \n",
    "\n",
    "Se cambia la clase Agente para que los Q-valores sean obtenidos con el perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agente(object):\n",
    "    \n",
    "    def __init__(self, entorno):\n",
    "        self.obs_dim=entorno.observation_space.shape              # dimension espacio observacion\n",
    "        self.obs_sup=entorno.observation_space.high               # limite superior\n",
    "        self.obs_inf=entorno.observation_space.low                # limite inferior\n",
    "        self.obs_bins=NUM_BINS                                    # discretizacion de un espacio continuo, numero de bins\n",
    "        self.ancho_bin=(self.obs_sup-self.obs_inf)/self.obs_bins  # ancho de cada parte de la discretizacion\n",
    "        self.dim_accion=entorno.action_space.n                    # dimension espacio accion\n",
    "        \n",
    "        self.Q=Perceptron(self.obs_dim, self.dim_accion)                    # Q-valores\n",
    "        self.Q_optimizador=torch.optim.Adam(self.Q.parameters(), lr=1e-5)   # optimizador Adam\n",
    "        \n",
    "        self.alfa=ALFA        # tasa de aprendizaje           \n",
    "        self.gamma=GAMMA      # factor de descuento\n",
    "        self.epsilon=EPSILON  # prob para escoger accion \n",
    "        \n",
    "        \n",
    "    def discretiza(self, obs):  # discretiza un espacio continuo\n",
    "        return tuple(((obs-self.obs_inf)/self.ancho_bin).astype(int))  # binning\n",
    "        \n",
    "        \n",
    "    def accion(self, obs):   # realiza la accion del agente\n",
    "        obs_discreta=self.discretiza(obs) # discretiza espacio\n",
    "        \n",
    "        # politica epsilon-greedy para escoger la accion (mejor probabilidad 1-eps, peor es epsilon)\n",
    "        if self.epsilon>MIN_EPSILON: self.epsilon-=DECAY_EPSILON\n",
    "            \n",
    "            \n",
    "        if np.random.random()>self.epsilon: \n",
    "            return np.argmax(self.Q(obs_discreta).data.to(torch.device('cpu')).numpy())\n",
    "        else: \n",
    "            return np.random.choice([a for a in range(self.dim_accion)])\n",
    "        \n",
    "        \n",
    "    def aprende(self, obs, acciona, recompensa, siguiente_obs): # metodo de aprendizaje\n",
    "        td_objetivo=recompensa+self.gamma*torch.max(self.Q(siguiente_obs))         # valor de aprendizaje\n",
    "        td_error=torch.nn.functional.mse_loss(self.Q(obs)[acciona], td_objetivo)   # error aprendizaje\n",
    "        self.Q_optimizador.zero_grad()\n",
    "        td_error.backward()\n",
    "        self.Q_optimizador.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejecucion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# primero las constantes (se pueden considerar hiperparametros)\n",
    "\n",
    "MIN_EPSILON=0.005                               # epsilon minimo, probabilidad minima para escoger una accion\n",
    "EPSILON=1.                                      # prob para escoger accion\n",
    "NUM_MAX_EPIS=50000                              # numero maximo de episodios\n",
    "PASOS_POR_EPI=200                               # pasos por episodio\n",
    "NUM_MAX_PASOS=NUM_MAX_EPIS*PASOS_POR_EPI        # numero maximo de pasos por episodio  \n",
    "DECAY_EPSILON=500*MIN_EPSILON/NUM_MAX_PASOS     # ajuste del valor de epsilon \n",
    "ALFA=0.05                                       # tasa de aprendizaje\n",
    "GAMMA=0.98                                      # factor de descuento\n",
    "NUM_BINS=20                                     # numero de binnings para discretizacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 0 acabado en 16 pasos..|..Recompensa:16.0, Recompensa media:16.0, Mejor Recompensa:16.0\n",
      "Episodio 2000 acabado en 14 pasos..|..Recompensa:14.0, Recompensa media:22.330834582708647, Mejor Recompensa:81.0\n",
      "Episodio 4000 acabado en 67 pasos..|..Recompensa:67.0, Recompensa media:22.331667083229192, Mejor Recompensa:102.0\n",
      "Episodio 6000 acabado en 20 pasos..|..Recompensa:20.0, Recompensa media:22.318280286618897, Mejor Recompensa:118.0\n",
      "Episodio 8000 acabado en 13 pasos..|..Recompensa:13.0, Recompensa media:22.340832395950507, Mejor Recompensa:118.0\n",
      "Episodio 10000 acabado en 13 pasos..|..Recompensa:13.0, Recompensa media:22.242975702429757, Mejor Recompensa:118.0\n",
      "Episodio 12000 acabado en 25 pasos..|..Recompensa:25.0, Recompensa media:22.254728772602284, Mejor Recompensa:118.0\n",
      "Episodio 14000 acabado en 14 pasos..|..Recompensa:14.0, Recompensa media:22.220341404185415, Mejor Recompensa:143.0\n",
      "Episodio 16000 acabado en 41 pasos..|..Recompensa:41.0, Recompensa media:22.194925317167677, Mejor Recompensa:143.0\n",
      "Episodio 18000 acabado en 26 pasos..|..Recompensa:26.0, Recompensa media:22.13304816399089, Mejor Recompensa:143.0\n",
      "Episodio 20000 acabado en 19 pasos..|..Recompensa:19.0, Recompensa media:22.071246437678116, Mejor Recompensa:143.0\n",
      "Episodio 22000 acabado en 19 pasos..|..Recompensa:19.0, Recompensa media:21.98581882641698, Mejor Recompensa:143.0\n",
      "Episodio 24000 acabado en 19 pasos..|..Recompensa:19.0, Recompensa media:21.964751468688803, Mejor Recompensa:161.0\n",
      "Episodio 26000 acabado en 14 pasos..|..Recompensa:14.0, Recompensa media:21.939233106418985, Mejor Recompensa:161.0\n",
      "Episodio 28000 acabado en 21 pasos..|..Recompensa:21.0, Recompensa media:21.86236205849791, Mejor Recompensa:161.0\n",
      "Episodio 30000 acabado en 11 pasos..|..Recompensa:11.0, Recompensa media:21.797106763107898, Mejor Recompensa:161.0\n",
      "Episodio 32000 acabado en 19 pasos..|..Recompensa:19.0, Recompensa media:21.701384331739632, Mejor Recompensa:161.0\n",
      "Episodio 34000 acabado en 10 pasos..|..Recompensa:10.0, Recompensa media:21.629010911443782, Mejor Recompensa:161.0\n",
      "Episodio 36000 acabado en 15 pasos..|..Recompensa:15.0, Recompensa media:21.575095136246215, Mejor Recompensa:161.0\n",
      "Episodio 38000 acabado en 23 pasos..|..Recompensa:23.0, Recompensa media:21.48551353911739, Mejor Recompensa:161.0\n",
      "Episodio 40000 acabado en 19 pasos..|..Recompensa:19.0, Recompensa media:21.40316492087698, Mejor Recompensa:161.0\n",
      "Episodio 42000 acabado en 15 pasos..|..Recompensa:15.0, Recompensa media:21.312230661174734, Mejor Recompensa:161.0\n",
      "Episodio 44000 acabado en 24 pasos..|..Recompensa:24.0, Recompensa media:21.223063112201995, Mejor Recompensa:161.0\n",
      "Episodio 46000 acabado en 16 pasos..|..Recompensa:16.0, Recompensa media:21.133910132388426, Mejor Recompensa:161.0\n",
      "Episodio 48000 acabado en 15 pasos..|..Recompensa:15.0, Recompensa media:21.05310306035291, Mejor Recompensa:161.0\n"
     ]
    }
   ],
   "source": [
    "# se ejecuta todo\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "if __name__=='__main__':\n",
    "    \n",
    "    entorno=gym.make('CartPole-v0')\n",
    "    \n",
    "    agente=Agente(entorno)\n",
    "    primer_episodio=True\n",
    "    recompensas_episodio=[]\n",
    "    \n",
    "    for episodio in range(NUM_MAX_EPIS):\n",
    "        observacion=entorno.reset()\n",
    "        recompensa_total=0.\n",
    "        \n",
    "        for paso in range(NUM_MAX_PASOS):\n",
    "            #entorno.render()\n",
    "            accion=agente.accion(observacion)\n",
    "            \n",
    "            siguiente_obs, recompensa, done, info=entorno.step(accion)\n",
    "            agente.aprende(observacion, accion, recompensa, siguiente_obs)\n",
    "            \n",
    "            observacion=siguiente_obs\n",
    "            recompensa_total+=recompensa\n",
    "            \n",
    "            if done:\n",
    "                if primer_episodio:\n",
    "                    max_recompensa=recompensa_total\n",
    "                    primer_episodio=False\n",
    "                recompensas_episodio.append(recompensa_total)\n",
    "                \n",
    "                if recompensa_total>max_recompensa:\n",
    "                    max_recompensa=recompensa_total\n",
    "                \n",
    "                if episodio%2000==0:\n",
    "                    print ('Episodio {} acabado en {} pasos..|..Recompensa:{}, Recompensa media:{}, Mejor Recompensa:{}'.format(episodio, paso+1, recompensa_total, np.mean(recompensas_episodio),max_recompensa))\n",
    "                \n",
    "                break\n",
    "        entorno.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import random\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from function_approximator.perceptron import SLP\n",
    "from utils.decay_schedule import LinearDecaySchedule\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "MAX_NUM_EPISODES = 10000\n",
    "MAX_STEPS_PER_EPISODE = 300\n",
    "\n",
    "\n",
    "class Shallow_Q_Learner(object):\n",
    "    def __init__(self,\n",
    "                 state_shape,\n",
    "                 action_shape,\n",
    "                 learning_rate=0.005,\n",
    "                 gamma=0.98):\n",
    "        self.state_shape = state_shape\n",
    "        self.action_shape = action_shape\n",
    "        self.gamma = gamma\n",
    "        self.learning_rate = learning_rate\n",
    "        self.Q = SLP(state_shape, action_shape)\n",
    "        self.Q_optimizer = torch.optim.Adam(self.Q.parameters(), lr=1e-3)\n",
    "        self.policy = self.epsilon_greedy_Q\n",
    "        self.epsilon_max = 1.0\n",
    "        self.epsilon_min = 0.05\n",
    "        self.epsilon_decay = LinearDecaySchedule(initial_value=self.epsilon_max,\n",
    "                                                 final_value=self.epsilon_min,\n",
    "                                                 max_steps=0.5*MAX_NUM_EPISODES * MAX_STEPS_PER_EPISODE)\n",
    "        self.step_num = 0\n",
    "\n",
    "    def get_action(self, observation):\n",
    "        return self.policy(observation)\n",
    "\n",
    "    def epsilon_greedy_Q(self, observation):\n",
    "        if random.random() < self.epsilon_decay(self.step_num):\n",
    "            action = random.choice([i for i in range(self.action_shape)])\n",
    "        else:\n",
    "            action = np.argmax(self.Q(observation).data.numpy())\n",
    "        return action\n",
    "\n",
    "    def learn(self, s, a, r, s_next):\n",
    "        td_target = r + self.gamma * torch.max(self.Q(s_next))\n",
    "        td_error = torch.nn.functional.mse_loss(self.Q(s)[a], td_target)\n",
    "        self.Q_optimizer.zero_grad()\n",
    "        td_error.backward()\n",
    "        self.Q_optimizer.step()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    observation_shape = env.observation_space.shape\n",
    "    action_shape = env.action_space.n\n",
    "    agent = Shallow_Q_Learner(observation_shape, action_shape)\n",
    "    first_episode = True\n",
    "    episode_rewards = list()\n",
    "    for episode in range(MAX_NUM_EPISODES):\n",
    "        obs = env.reset()\n",
    "        cum_reward = 0.0\n",
    "        for step in range(MAX_STEPS_PER_EPISODE):\n",
    "            action = agent.get_action(obs)\n",
    "            next_obs, reward, done, info = env.step(action)\n",
    "            agent.learn(obs, action, reward, next_obs)\n",
    "\n",
    "            obs = next_obs\n",
    "            cum_reward += reward\n",
    "\n",
    "            if done:\n",
    "                if first_episode:\n",
    "                    max_reward = cum_reward\n",
    "                    first_episode = False\n",
    "                episode_rewards.append(cum_reward)\n",
    "                if cum_reward > max_reward:\n",
    "                    max_reward = cum_reward\n",
    "                print('\\n\\033[94mEpisode#{} ended in {} steps. \\033[91m reward = {}'.format(\n",
    "                    episode, step+1, cum_reward),end='', flush=True)\n",
    "                print('\\033[95m mean_reward={} best_reward={}'.format(np.mean(episode_rewards), max_reward),\n",
    "                      end='', flush=True)\n",
    "                break\n",
    "        env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
