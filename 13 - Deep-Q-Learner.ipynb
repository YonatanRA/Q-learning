{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agente con CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "from UtilsDQL import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Deep_Q_Learner(object):\n",
    "    \n",
    "    def __init__(self, dim_estado, dim_accion, params):\n",
    "        \"\"\"\n",
    "        self.Q is the Action-Value function. This agent represents Q using a Neural Network\n",
    "        If the input is a single dimensional vector, uses a Single-Layer-Perceptron else if the input is 3 dimensional\n",
    "        image, use a Convolutional-Neural-Network\n",
    "\n",
    "        :param state_shape: Shape (tuple) of the observation/state\n",
    "        :param action_shape: Shape (number) of the discrete action space\n",
    "        :param params: A dictionary containing various Agent configuration parameters and hyper-parameters\n",
    "        \"\"\"\n",
    "        self.dim_estado=dim_estado\n",
    "        self.dim_accion=dim_accion\n",
    "        self.params=params\n",
    "        self.gamma=self.params['gamma']             # Agent's discount factor\n",
    "        self.tasa_apredizaje=self.params['lr']      # Agent's Q-learning rate\n",
    "        self.mejor_recompensa_media=-float('inf')   # Agent's personal best mean episode reward\n",
    "        self.mejor_recompensa=-float('inf')\n",
    "        self.pasos_entrenamiento=0                  # Number of training batch steps completed so far\n",
    "\n",
    "        if len(self.dim_estado)==1:                 # Single dimensional observation/state space\n",
    "            self.DQN=Perceptron\n",
    "        elif len(self.dim_estado)==3:               # 3D/image observation/state\n",
    "            self.DQN=CNN\n",
    "\n",
    "        self.Q=self.DQN(dim_estado, dim_accion, device).to(device)\n",
    "        self.Q.apply(xavier)\n",
    "\n",
    "        self.Q_optimizador=torch.optim.Adam(self.Q.parameters(), lr=self.tasa_apredizaje)\n",
    "        \n",
    "        if self.params['use_target_network']:\n",
    "            self.Q_objetivo=self.DQN(dim_estado, dim_accion, device).to(device)\n",
    "        # self.policy is the policy followed by the agent. This agents follows\n",
    "        # an epsilon-greedy policy w.r.t it's Q estimate.\n",
    "        self.politica=self.epsilon_greedy_Q\n",
    "        self.epsilon_max=params['epsilon_max']\n",
    "        self.epsilon_min=params['epsilon_min']\n",
    "        self.epsilon_decay=DecayLineal(valor_ini=self.epsilon_max,\n",
    "                                       valor_final=self.epsilon_min,\n",
    "                                       pasos_max=self.params['epsilon_decay_final_step'])\n",
    "        self.num_pasos=0\n",
    "\n",
    "        self.memoria=Memoria(capacidad=int(self.params['experience_memory_capacity']))  # Initialize an Experience memory with 1M capacity\n",
    "\n",
    "    def accion(self, observacion):\n",
    "        observacion=np.array(observacion)  # Observations could be lazy frames. So force fetch before moving forward\n",
    "        observacion=observacion/255.       # Scale/Divide by max limit of obs' dtype. 255 for uint8\n",
    "        if len(observacion.shape)==3:      # Single image (not a batch)\n",
    "            if observacion.shape[2]<observacion.shape[0]:  # Probably observation is in W x H x C format\n",
    "                # NOTE: This is just an additional check. The env wrappers are taking care of this conversion already\n",
    "                # Reshape to C x H x W format as per PyTorch's convention\n",
    "                observacion=observacion.reshape(observacion.shape[2], observacion.shape[1], observacion.shape[0])\n",
    "            observacion=np.expand_dims(observacion, 0)  # Create a batch dimension\n",
    "        return self.politica(observacion)\n",
    "\n",
    "    def epsilon_greedy_Q(self, observacion):\n",
    "        # Decay Epsilon/exploration as per schedule\n",
    "        writer.add_scalar('DQL/epsilon', self.epsilon_decay(self.num_pasos), self.num_pasos)\n",
    "        self.num_pasos+=1\n",
    "        if random.random()<self.epsilon_decay(self.num_pasos) and not self.params['test']:\n",
    "            accion=random.choice([i for i in range(self.dim_accion)])\n",
    "        else:\n",
    "            accion=np.argmax(self.Q(observacion).data.to(torch.device('cpu')).numpy())\n",
    "        return accion\n",
    "\n",
    "    def aprende(self, s, a, r, s_next, done):\n",
    "        # TD(0) Q-learning\n",
    "        if done:  # End of episode\n",
    "            td_objetivo=recompensa+0.  # Set the value of terminal state to zero\n",
    "        else:\n",
    "            td_objetivo=r+self.gamma*torch.max(self.Q(s_next))\n",
    "        td_error=td_objetivo-self.Q(s)[a]\n",
    "        # Update Q estimate\n",
    "        #self.Q(s)[a] = self.Q(s)[a] + self.learning_rate * td_error\n",
    "        self.Q_optimizador.zero_grad()\n",
    "        td_error.backward()\n",
    "        self.Q_optimizador.step()\n",
    "\n",
    "    def learn_from_batch_experience(self, experiences):\n",
    "        batch_xp = Experience(*zip(*experiences))\n",
    "        obs_batch = np.array(batch_xp.obs) / 255.0  # Scale/Divide by max limit of obs's dtype. 255 for uint8\n",
    "        action_batch = np.array(batch_xp.action)\n",
    "        reward_batch = np.array(batch_xp.reward)\n",
    "        # Clip the rewards\n",
    "        if self.params[\"clip_rewards\"]:\n",
    "            reward_batch = np.sign(reward_batch)\n",
    "        next_obs_batch = np.array(batch_xp.next_obs) / 255.0  # Scale/Divide by max limit of obs' dtype. 255 for uint8\n",
    "        done_batch = np.array(batch_xp.done)\n",
    "\n",
    "        if self.params['use_target_network']:\n",
    "            #if self.training_steps_completed % self.params['target_network_update_freq'] == 0:\n",
    "            if self.step_num % self.params['target_network_update_freq'] == 0:\n",
    "                # The *update_freq is the Num steps after which target net is updated.\n",
    "                # A schedule can be used instead to vary the update freq.\n",
    "                self.Q_target.load_state_dict(self.Q.state_dict())\n",
    "            td_target = reward_batch + ~done_batch * \\\n",
    "                np.tile(self.gamma, len(next_obs_batch)) * \\\n",
    "                self.Q_target(next_obs_batch).max(1)[0].data.cpu().numpy()\n",
    "        else:\n",
    "            td_target = reward_batch + ~done_batch * \\\n",
    "                np.tile(self.gamma, len(next_obs_batch)) * \\\n",
    "                self.Q(next_obs_batch).detach().max(1)[0].data.cpu().numpy()\n",
    "\n",
    "        td_target = torch.from_numpy(td_target).to(device)\n",
    "        action_idx = torch.from_numpy(action_batch).to(device)\n",
    "        td_error = torch.nn.functional.mse_loss( self.Q(obs_batch).gather(1, action_idx.view(-1, 1)),\n",
    "                                                       td_target.float().unsqueeze(1))\n",
    "\n",
    "        self.Q_optimizer.zero_grad()\n",
    "        td_error.mean().backward()\n",
    "        writer.add_scalar(\"DQL/td_error\", td_error.mean(), self.step_num)\n",
    "        self.Q_optimizer.step()\n",
    "\n",
    "    def replay_experience(self, batch_size = None):\n",
    "        batch_size = batch_size if batch_size is not None else self.params['replay_batch_size']\n",
    "        experience_batch = self.memory.sample(batch_size)\n",
    "        self.learn_from_batch_experience(experience_batch)\n",
    "        self.training_steps_completed += 1  # Increment the number of training batch steps complemented\n",
    "\n",
    "    def save(self, env_name):\n",
    "        file_name = self.params['save_dir'] + \"DQL_\" + env_name + \".ptm\"\n",
    "        agent_state = {\"Q\": self.Q.state_dict(),\n",
    "                       \"best_mean_reward\": self.best_mean_reward,\n",
    "                       \"best_reward\": self.best_reward};\n",
    "        torch.save(agent_state, file_name)\n",
    "        print(\"Agent's state saved to \", file_name)\n",
    "\n",
    "    def load(self, env_name):\n",
    "        file_name = self.params['load_dir'] + \"DQL_\" + env_name + \".ptm\"\n",
    "        agent_state = torch.load(file_name, map_location= lambda storage, loc: storage)\n",
    "        self.Q.load_state_dict(agent_state[\"Q\"])\n",
    "        self.Q.to(device)\n",
    "        self.best_mean_reward = agent_state[\"best_mean_reward\"]\n",
    "        self.best_reward = agent_state[\"best_reward\"]\n",
    "        print(\"Loaded Q model state from\", file_name,\n",
    "              \" which fetched a best mean reward of:\", self.best_mean_reward,\n",
    "              \" and an all time best reward of:\", self.best_reward)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
