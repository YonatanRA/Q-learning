{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agente con CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from UtilsDQL.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "from UtilsDQL import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Deep_Q_Learner(object):\n",
    "    \n",
    "    def __init__(self, dim_estado, dim_accion, params):\n",
    "        '''\n",
    "        self.Q es la funcion Accion-Valor. Este agente representa los Q-valores usando un red neuronal.\n",
    "        Si la entrada es vector de 1 dimension se usara el perceptron, si tiene 3 se usara la CNN\n",
    "        '''\n",
    "        \n",
    "        self.dim_estado=dim_estado\n",
    "        self.dim_accion=dim_accion\n",
    "        self.params=params                          # diccionario de parametros\n",
    "        self.gamma=self.params['gamma']             # factor de descuento del agente\n",
    "        self.tasa_apredizaje=self.params['lr']      # tasa de aprendizaje del agente\n",
    "        self.mejor_recompensa_media=-float('inf')   # recompensas por episodio\n",
    "        self.mejor_recompensa=-float('inf')\n",
    "        self.pasos_entrenamiento=0                  # numero de pasos del batch de entrenamiento realizados\n",
    "\n",
    "        if len(self.dim_estado)==1:                 \n",
    "            self.DQN=Perceptron\n",
    "        elif len(self.dim_estado)==3:              \n",
    "            self.DQN=CNN\n",
    "\n",
    "        self.Q=self.DQN(dim_estado, dim_accion, device).to(device)\n",
    "        self.Q.apply(xavier)\n",
    "\n",
    "        self.Q_optimizador=torch.optim.Adam(self.Q.parameters(), lr=self.tasa_apredizaje)\n",
    "        \n",
    "        if self.params['use_target_network']:\n",
    "            self.Q_objetivo=self.DQN(dim_estado, dim_accion, device).to(device)\n",
    "\n",
    "        self.politica=self.epsilon_greedy_Q        # se sigue la politica e-greedy\n",
    "        self.epsilon_max=params['epsilon_max']\n",
    "        self.epsilon_min=params['epsilon_min']\n",
    "        self.epsilon_decay=DecayLineal(valor_ini=self.epsilon_max,\n",
    "                                       valor_final=self.epsilon_min,\n",
    "                                       pasos_max=self.params['epsilon_decay_final_step'])\n",
    "        self.num_pasos=0\n",
    "\n",
    "        self.memoria=Memoria(capacidad=int(self.params['experience_memory_capacity']))  # inicializa la memoria\n",
    "\n",
    "        \n",
    "    def accion(self, observacion):\n",
    "        observacion=np.array(observacion)  # frames de observacion\n",
    "        observacion=observacion/255.       # normalizacion del frame\n",
    "        if len(observacion.shape)==3:      # una sola imagen, no el batch\n",
    "            if observacion.shape[2]<observacion.shape[0]:  # se cambia de dimension a Canal X Alto X Ancho\n",
    "                observacion=observacion.reshape(observacion.shape[2], observacion.shape[1], observacion.shape[0])\n",
    "            observacion=np.expand_dims(observacion, 0)  # se expande la dimension para el batch\n",
    "        return self.politica(observacion)\n",
    "\n",
    "    \n",
    "    def epsilon_greedy_Q(self, observacion):\n",
    "        writer.add_scalar('DQL/epsilon', self.epsilon_decay(self.num_pasos), self.num_pasos)\n",
    "        self.num_pasos+=1\n",
    "        if random.random()<self.epsilon_decay(self.num_pasos) and not self.params['test']:\n",
    "            accion=random.choice([i for i in range(self.dim_accion)])\n",
    "        else:\n",
    "            accion=np.argmax(self.Q(observacion).data.to(torch.device('cpu')).numpy())\n",
    "        return accion\n",
    "\n",
    "    \n",
    "    def aprende(self, s, a, r, s_next, done):\n",
    "        if done:  # fin del episodio\n",
    "            td_objetivo=recompensa+0.  \n",
    "        else:\n",
    "            td_objetivo=r+self.gamma*torch.max(self.Q(s_next))\n",
    "        td_error=td_objetivo-self.Q(s)[a]\n",
    "        # actualiza la estimacion de Q\n",
    "        #self.Q(s)[a]=self.Q(s)[a]+self.tasa_aprendizaje*td_error\n",
    "        self.Q_optimizador.zero_grad()\n",
    "        td_error.backward()\n",
    "        self.Q_optimizador.step()\n",
    "\n",
    "        \n",
    "    def aprende_de_experiencia(self, experiencias):\n",
    "        batch_xp=Experiencia(*zip(*experiencias))\n",
    "        obs_batch=np.array(batch_xp.obs)/255.  \n",
    "        accion_batch=np.array(batch_xp.action)\n",
    "        recompensa_batch=np.array(batch_xp.reward)\n",
    "      \n",
    "        if self.params['clip_rewards']:\n",
    "            recompensa_batch=np.sign(recompensa_batch)\n",
    "        next_obs_batch=np.array(batch_xp.next_obs)/255.  \n",
    "        done_batch=np.array(batch_xp.done)\n",
    "\n",
    "        if self.params['use_target_network']:\n",
    "            if self.num_pasos%self.params['target_network_update_freq']==0:\n",
    "                self.Q_objetivo.load_state_dict(self.Q.state_dict())\n",
    "                \n",
    "            td_objetivo=recompensa_batch+~done_batch* \\\n",
    "                np.tile(self.gamma, len(next_obs_batch))* \\\n",
    "                self.Q_objetivo(next_obs_batch).max(1)[0].data.cpu().numpy()\n",
    "        else:\n",
    "            td_objetivo=recompensa_batch+~done_batch* \\\n",
    "                np.tile(self.gamma, len(next_obs_batch))* \\\n",
    "                self.Q(next_obs_batch).detach().max(1)[0].data.cpu().numpy()\n",
    "\n",
    "        td_objetivo=torch.from_numpy(td_objetivo).to(device)\n",
    "        accion_idx=torch.from_numpy(accion_batch).to(device)\n",
    "        td_error=torch.nn.functional.mse_loss(self.Q(obs_batch).gather(1, accion_idx.view(-1, 1)),\n",
    "                                              td_objetivo.float().unsqueeze(1))\n",
    "\n",
    "        self.Q_optimizador.zero_grad()\n",
    "        td_error.mean().backward()\n",
    "        writer.add_scalar('DQL/td_error', td_error.mean(), self.num_pasos)\n",
    "        self.Q_optimizador.step()\n",
    "\n",
    "        \n",
    "    def replay_experiencia(self, batch_size=None):\n",
    "        batch_size=batch_size if batch_size is not None else self.params['replay_batch_size']\n",
    "        experiencia_batch=self.memoria.sample(batch_size)\n",
    "        self.aprende_de_experiencia(experiencia_batch)\n",
    "        self.pasos_entrenamiento+=1  \n",
    "\n",
    "        \n",
    "    def guardar(self, nombre_ent):\n",
    "        archivo=self.params['save_dir']+'DQL_'+nombre_ent+'.ptm'\n",
    "        estado_agente={'Q': self.Q.state_dict(),\n",
    "                       'best_mean_reward': self.mejor_recompensa_media,\n",
    "                       'best_reward': self.recompensa_media};\n",
    "        torch.save(estado_agente, archivo)\n",
    "        print('Estado del agente guardado en ', archivo)\n",
    "\n",
    "        \n",
    "    def cargar(self, nombre_ent):\n",
    "        archivo=self.params['load_dir']+'DQL_'+nombre_ent+'.ptm'\n",
    "        estado_agente=torch.load(archivo, map_location= lambda x, loc: x)\n",
    "        \n",
    "        self.Q.load_state_dict(estado_agente['Q'])\n",
    "        self.Q.to(device)\n",
    "        self.mejor_recompensa_media=estado_agente['best_mean_reward']\n",
    "        self.recompensa_media=estado_agente['best_reward']\n",
    "        \n",
    "        print('Cargado el estado del Q modelo desde', archivo,\n",
    "              ' con un mejor recompensa media de:', self.mejor_recompensa_media,\n",
    "              ' y una mejor recompensa de :', self.recompensa_media)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejecutando el agente...\n",
    "\n",
    "Hay que pasarlo a un archivo .py para ejecucion."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "args=ArgumentParser('deep_Q_learner')\n",
    "\n",
    "args.add_argument('--params', help='Ruta al archivo json de parametros. Por defecto es parameters.json',\n",
    "                  default='parameters.json', metavar='PFILE')\n",
    "\n",
    "args.add_argument('--env', help='ID del entorno Atari disponible en OpenAI Gym. Por defecto SeaquestNoFrameskip-v4',\n",
    "                  default='SeaquestNoFrameskip-v4', metavar='ENV')\n",
    "\n",
    "args.add_argument('--gpu-id', help='ID de la GPU. Por defecto 0', default=0, type=int, metavar='GPU_ID')\n",
    "\n",
    "args.add_argument('--render', help='Renderiza por pantalla el entorno. Por defecto apagado', \n",
    "                  action='store_true', default=False)\n",
    "\n",
    "args.add_argument('--test', help='Modo Test. Juega sin aprender. Por defecto apagado', \n",
    "                  action='store_true',\n",
    "                  default=False)\n",
    "\n",
    "args.add_argument('--record', help='Permite grabar la performance del agente, video y stats.',\n",
    "                  action='store_true', default=False)\n",
    "\n",
    "args.add_argument('--recording-output-dir', help='Directorio para guardar salidas. Por defecto=./modelos_entrenados/resultados',\n",
    "                  default='./modelos_entrenados/resultados')\n",
    "\n",
    "args=args.parse_args()\n",
    "\n",
    "\n",
    "\n",
    "params_manager=ParamControl(args.params)\n",
    "semilla=params_manager.param_agente()['seed']\n",
    "\n",
    "ruta_prefix=params_manager.param_agente()['summary_file_path_prefix']\n",
    "ruta=ruta_prefix+args.env+'_'+datetime.now().strftime('%y-%m-%d-%H-%M')\n",
    "escritor=SummaryWriter(ruta)\n",
    "\n",
    "\n",
    "# Exporta los parametros a un json para tener log de cada experimento\n",
    "params_manager.exporta_param_entorno(ruta+ '/'+ 'ent_params.json')\n",
    "params_manager.exporta_param_agente(ruta+ '/'+ 'agente_params.json')\n",
    "\n",
    "num_pasos_global=0\n",
    "usa_cuda=params_manager.param_agente()['use_cuda']\n",
    "\n",
    "\n",
    "device=torch.device('cuda:' + str(args.gpu_id) if torch.cuda.is_available() and use_cuda else 'cpu')\n",
    "torch.manual_seed(semilla)\n",
    "np.random.seed(semilla)\n",
    "\n",
    "if torch.cuda.is_available() and use_cuda:\n",
    "    print ('cuda')\n",
    "    torch.cuda.manual_seed_all(semilla)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    ent_conf=params_manager.param_entorno()\n",
    "    ent_conf['env_name']=args.env\n",
    "    \n",
    "    # In test mode, let the end of the game be the end of episode \n",
    "    # rather than ending episode at the end of every life.\n",
    "    # This helps to report out the (mean and max) episode rewards per game (rather than per life!)\n",
    "    if args.test:\n",
    "        env_conf[\"episodic_life\"] = False\n",
    "        \n",
    "    # Specify the reward calculation type used for printing stats at the end of every episode.\n",
    "    # If \"episode_life\" is true, the printed stats (reward, mean reward, max reward) are per life. If \"episodic_life\"\n",
    "    # is false, the printed stats/scores are per game in Atari environments\n",
    "    rew_type = \"LIFE\" if env_conf[\"episodic_life\"] else \"GAME\"\n",
    "\n",
    "    # If a custom useful_region configuration for this environment ID is available, use it if not use the Default\n",
    "    custom_region_available = False\n",
    "    for key, value in env_conf['useful_region'].items():\n",
    "        if key in args.env:\n",
    "            env_conf['useful_region'] = value\n",
    "            custom_region_available = True\n",
    "            break\n",
    "    if custom_region_available is not True:\n",
    "        env_conf['useful_region'] = env_conf['useful_region']['Default']\n",
    "\n",
    "    print(\"Using env_conf:\", env_conf)\n",
    "    atari_env = False\n",
    "    for game in Atari.get_games_list():\n",
    "        if game.replace(\"_\", \"\") in args.env.lower():\n",
    "            atari_env = True\n",
    "    if atari_env:\n",
    "        env = Atari.make_env(args.env, env_conf)\n",
    "    else:\n",
    "        print(\"Given environment name is not an Atari Env. Creating a Gym env\")\n",
    "        # Resize the obs to w x h (84 x 84 by default) and then reshape it to be in the C x H x W format\n",
    "        env = env_utils.ResizeReshapeFrames(gym.make(args.env))\n",
    "\n",
    "    if args.record:  # If monitor is enabled, record stats and video of agent's performance\n",
    "        env = gym.wrappers.Monitor(env, args.recording_output_dir, force=True)\n",
    "\n",
    "    observation_shape = env.observation_space.shape\n",
    "    action_shape = env.action_space.n\n",
    "    agent_params = params_manager.get_agent_params()\n",
    "    agent_params[\"test\"] = args.test\n",
    "    agent = Deep_Q_Learner(observation_shape, action_shape, agent_params)\n",
    "\n",
    "    episode_rewards = list()\n",
    "    prev_checkpoint_mean_ep_rew = agent.best_mean_reward\n",
    "    num_improved_episodes_before_checkpoint = 0  # To keep track of the num of ep with higher perf to save model\n",
    "    print(\"Using agent_params:\", agent_params)\n",
    "    if agent_params['load_trained_model']:\n",
    "        try:\n",
    "            agent.load(env_conf[\"env_name\"])\n",
    "            prev_checkpoint_mean_ep_rew = agent.best_mean_reward\n",
    "        except FileNotFoundError:\n",
    "            print(\"WARNING: No trained model found for this environment. Training from scratch.\")\n",
    "\n",
    "    #for episode in range(agent_params['max_num_episodes']):\n",
    "    episode = 0\n",
    "    while global_step_num <= agent_params['max_training_steps']:\n",
    "        obs = env.reset()\n",
    "        cum_reward = 0.0  # Cumulative reward\n",
    "        done = False\n",
    "        step = 0\n",
    "        #for step in range(agent_params['max_steps_per_episode']):\n",
    "        while not done:\n",
    "            if env_conf['render'] or args.render:\n",
    "                env.render()\n",
    "            action = agent.get_action(obs)\n",
    "            next_obs, reward, done, info = env.step(action)\n",
    "            #agent.learn(obs, action, reward, next_obs, done)\n",
    "            agent.memory.store(Experience(obs, action, reward, next_obs, done))\n",
    "\n",
    "            obs = next_obs\n",
    "            cum_reward += reward\n",
    "            step += 1\n",
    "            global_step_num +=1\n",
    "\n",
    "            if done is True:\n",
    "                episode += 1\n",
    "                episode_rewards.append(cum_reward)\n",
    "                if cum_reward > agent.best_reward:\n",
    "                    agent.best_reward = cum_reward\n",
    "                if np.mean(episode_rewards) > prev_checkpoint_mean_ep_rew:\n",
    "                    num_improved_episodes_before_checkpoint += 1\n",
    "                if num_improved_episodes_before_checkpoint >= agent_params[\"save_freq_when_perf_improves\"]:\n",
    "                    prev_checkpoint_mean_ep_rew = np.mean(episode_rewards)\n",
    "                    agent.best_mean_reward = np.mean(episode_rewards)\n",
    "                    agent.save(env_conf['env_name'])\n",
    "                    num_improved_episodes_before_checkpoint = 0\n",
    "                print(\"\\nEpisode#{} ended in {} steps. Per {} stats: reward ={} ; mean_reward={:.3f} best_reward={}\".\n",
    "                      format(episode, step+1, rew_type, cum_reward, np.mean(episode_rewards), agent.best_reward))\n",
    "                writer.add_scalar(\"main/ep_reward\", cum_reward, global_step_num)\n",
    "                writer.add_scalar(\"main/mean_ep_reward\", np.mean(episode_rewards), global_step_num)\n",
    "                writer.add_scalar(\"main/max_ep_rew\", agent.best_reward, global_step_num)\n",
    "                # Learn from batches of experience once a certain amount of xp is available unless in test only mode\n",
    "                if agent.memory.get_size() >= 2 * agent_params['replay_start_size'] and not args.test:\n",
    "                    agent.replay_experience()\n",
    "\n",
    "                break\n",
    "    env.close()\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
